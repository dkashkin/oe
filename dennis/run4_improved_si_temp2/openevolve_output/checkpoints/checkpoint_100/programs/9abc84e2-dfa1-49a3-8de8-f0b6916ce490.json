{"id": "9abc84e2-dfa1-49a3-8de8-f0b6916ce490", "code": "An elegant and highly effective approach to boosting the fitness score for this packing problem is to introduce a secondary, precise non-linear optimization phase. \n\nWhile the Adam optimizer is excellent at aggressively moving the 26 circles to find a macro-scale non-overlapping layout, it struggles with extremely fine numerical precision constraints at convergence (vanishing gradients). To bridge this gap, we can take the most promising candidate configurations outputted by Adam and feed them into Scipy's Sequential Least Squares Programming (SLSQP). We supply SLSQP with exact, mathematically precise Jacobian derivatives. This allows the solver to slide perfectly along the tight constraint bounds (packing the circles flush against each other and the walls) and fully extract the maximum mathematically possible radius configuration. \n\nLastly, we ensure that absolute constraint validity is maintained by feeding the SLSQP output through a final Linear Programming trim.\n\n### Improved Program Code\n\n```python\nimport numpy as np\nfrom scipy.optimize import linprog, minimize\n\ndef get_slsqp_funcs(N):\n    \"\"\"\n    Constructs and returns the exact objective and constraint functions\n    (along with their Jacobians) needed for precise SLSQP optimization.\n    \"\"\"\n    idx_i, idx_j = np.triu_indices(N, 1)\n    \n    def obj(v):\n        return -np.sum(v[2*N:])\n        \n    def jac_obj(v):\n        j = np.zeros_like(v)\n        j[2*N:] = -1.0\n        return j\n\n    def constr(v):\n        x = v[:N]\n        y = v[N:2*N]\n        r = v[2*N:]\n        \n        # Boundary constraints\n        c_bounds = np.concatenate([\n            x - r,\n            1.0 - x - r,\n            y - r,\n            1.0 - y - r\n        ])\n        \n        # Overlap constraints: dx^2 + dy^2 >= (r_i + r_j)^2\n        dx = x[idx_i] - x[idx_j]\n        dy = y[idx_i] - y[idx_j]\n        sr = r[idx_i] + r[idx_j]\n        c_overlap = dx**2 + dy**2 - sr**2\n        \n        return np.concatenate([c_bounds, c_overlap])\n        \n    def constr_jac(v):\n        x = v[:N]\n        y = v[N:2*N]\n        r = v[2*N:]\n        \n        J = np.zeros((4*N + len(idx_i), 3*N))\n        rng = np.arange(N)\n        \n        # Gradients for boundary constraints\n        J[rng, rng] = 1.0\n        J[rng, 2*N + rng] = -1.0\n        \n        J[N + rng, rng] = -1.0\n        J[N + rng, 2*N + rng] = -1.0\n        \n        J[2*N + rng, N + rng] = 1.0\n        J[2*N + rng, 2*N + rng] = -1.0\n        \n        J[3*N + rng, N + rng] = -1.0\n        J[3*N + rng, 2*N + rng] = -1.0\n        \n        # Gradients for pairwise distance constraints\n        dx = x[idx_i] - x[idx_j]\n        ", "changes_description": null, "language": "python", "parent_id": "b3e4e50e-f0f5-4e08-8caf-b3c4af7248fa", "generation": 5, "timestamp": 1771877793.0486856, "iteration_found": 90, "metrics": {"validity": 0.0, "combined_score": 0.0, "radius_variance": 0.0, "spatial_spread": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"validity": 1.0, "sum_radii": 2.63087295185359, "target_ratio": 0.9984337578191993, "combined_score": 0.9984337578191993, "radius_variance": 0.004493788267286105, "spatial_spread": 0.19209106056641073, "eval_time": 53.914138078689575}, "island": 2}, "prompts": {"full_rewrite_user": {"system": "You are an expert mathematician and Python developer specializing in computational geometry and circle packing optimization.\nYour task is to improve the given Python algorithm for finding a highly optimized arrangement of 26 circles within a 1x1 unit square. The objective is to maximize the sum of their radii without any of the circles overlapping or extending outside the boundaries of the square.\nInstead of hardcoding a direct arrangement, implement an iterative optimization algorithm to find the solution.\nWhen designing the optimization routine, incorporate the following geometric heuristics:\n* Seed initial positions strategically: Bias initial placements toward corners and edges to maximize space utilization.\n* Break perfect symmetry: Introduce slight random perturbations during the optimization to escape local maxima caused by edge constraints.\n* Tune optimization parameters: Ensure your physics model uses a decaying learning rate or simulated annealing approach to smoothly settle into the tightest possible packing.\n* Size placement: Bias the initialization to push larger circles toward the center and smaller circles into the corners and interstitial gaps.\n\nCode Requirements:\n* Completeness: You MUST provide a fully runnable script. Do not use ellipses (`...`), comments-as-placeholders, or unclosed structures.\n* Style: Ensure the code is syntactically perfect and strictly adheres to PEP 8 formatting.\n* Code review: doublecheck your code to make sure it will not bump into any runtime exceptions.\n* Response Format: response in plain text, without wrapping the code into a Markdown codeblock.\n", "user": "# Current Program Information\n- Fitness: 0.9984\n- Feature coordinates: \n- Focus areas: - Fitness improved: 0.9982 \u2192 0.9984\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### execution_time\n```\n53.91s\n```\n\n### stage\n```\nquick_validation\n```\n\n### packing_summary\n```\nSum of radii: 2.630873/2.635 = 0.9984\n```\n\n### validation_report\n```\nValid: True, Violations: 0 boundary, 0 overlaps\n```\n\n### stdout\n```\nExcellent packing! Achieved 99.8% of target value\n```\n\n### radius_stats\n```\nMin: 0.069515, Max: 0.137936, Avg: 0.101187\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Full rewrite\n- Metrics: validity: 1.0000, sum_radii: 2.6302, target_ratio: 0.9982, combined_score: 0.9982, radius_variance: 0.0045, spatial_spread: 0.1879, eval_time: 37.1331\n- Outcome: Improvement in all metrics\n\n### Attempt 2\n- Changes: Full rewrite\n- Metrics: validity: 1.0000, sum_radii: 2.6303, target_ratio: 0.9982, combined_score: 0.9982, radius_variance: 0.0049, spatial_spread: 0.1959, eval_time: 19.3155\n- Outcome: Improvement in all metrics\n\n### Attempt 1\n- Changes: Full rewrite\n- Metrics: validity: 1.0000, sum_radii: 2.6309, target_ratio: 0.9984, combined_score: 0.9984, radius_variance: 0.0045, spatial_spread: 0.1921, eval_time: 53.9141\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9984)\n```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\n\ndef construct_packing():\n    \"\"\"\n    Construct an optimized arrangement of 26 circles in a unit square\n    to maximize the sum of their radii.\n\n    Returns:\n        Tuple of (centers, radii, sum_of_radii)\n        centers: np.array of shape (26, 2) with (x, y) coordinates\n        radii: np.array of shape (26) with radius of each circle\n        sum_of_radii: Sum of all radii\n    \"\"\"\n    np.random.seed(42)\n    B = 96  # Batch size for parallel exploration\n    N = 26  # Number of circles\n\n    centers = np.random.uniform(0.1, 0.9, size=(B, N, 2))\n\n    # Pattern 1: Edge/Corner biased (Beta distribution)\n    centers[0:16] = np.random.beta(0.3, 0.3, size=(16, N, 2))\n\n    # Pattern 2: Concentric rings placement\n    for b in range(16, 32):\n        centers[b, 0] = [0.5, 0.5]\n        idx = 1\n        angle_offset_1 = np.random.uniform(0, 2 * np.pi)\n        for i in range(7):\n            theta = 2.0 * np.pi * i / 7.0 + angle_offset_1\n            centers[b, idx] = [0.5 + 0.22 * np.cos(theta),\n                               0.5 + 0.22 * np.sin(theta)]\n            idx += 1\n        angle_offset_2 = np.random.uniform(0, 2 * np.pi)\n        for i in range(18):\n            theta = 2.0 * np.pi * i / 18.0 + angle_offset_2\n            centers[b, idx] = [0.5 + 0.44 * np.cos(theta),\n                               0.5 + 0.44 * np.sin(theta)]\n            idx += 1\n        centers[b] += np.random.normal(0, 0.005, size=(N, 2))\n\n    # Pattern 3: Golden ratio spiral (Fibonacci)\n    phi = (1.0 + np.sqrt(5.0)) / 2.0\n    for b in range(32, 48):\n        angle_offset = np.random.uniform(0, 2 * np.pi)\n        for i in range(N):\n            r_dist = np.sqrt((i + 0.5) / N) * 0.45\n            theta = 2.0 * np.pi * i / phi + angle_offset\n            centers[b, i, 0] = 0.5 + r_dist * np.cos(theta)\n            centers[b, i, 1] = 0.5 + r_dist * np.sin(theta)\n        centers[b] += np.random.normal(0, 0.01, size=(N, 2))\n\n    # Pattern 4: Hexagonal grid roughly tailored for square packing\n    hex_centers = []\n    xv, yv = np.meshgrid(np.linspace(0.05, 0.95, 7), np.linspace(0.05, 0.95, 7))\n    xv[1::2] += (0.9 / 6) / 2.0  # Shift alternate rows\n    hex_pts = np.stack([xv.flatten(), yv.flatten()], axis=-1)\n    hex_pts = hex_pts[(hex_pts[:, 0] <= 0.95) & (hex_pts[:, 1] <= 0.95)]\n    for b in range(48, 64):\n        idx = np.random.choice(len(hex_pts), N, replace=False)\n        centers[b] = hex_pts[idx] + np.random.normal(0, 0.005, size=(N, 2))\n\n    # Pattern 5: Standard uniform grid\n    xv, yv = np.meshgrid(np.linspace(0.08, 0.92, 6), np.linspace(0.08, 0.92, 6))\n    grid_pts = np.stack([xv.flatten(), yv.flatten()], axis=-1)\n    for b in range(64, 80):\n        idx = np.random.choice(len(grid_pts), N, replace=False)\n        centers[b] = grid_pts[idx] + np.random.normal(0, 0.01, size=(N, 2))\n\n    # Pattern 6: Random uniform with noise\n    centers[80:96] = np.random.uniform(0.05, 0.95, size=(16, N, 2))\n\n    # Keep all centers in a safe bounding box initially\n    np.clip(centers, 0.02, 0.98, out=centers)\n\n    # Size placement bias: larger radii in the center, smaller in corners/edges\n    dist_to_center = np.linalg.norm(centers - 0.5, axis=-1)\n    radii = 0.08 - 0.05 * (dist_to_center / 0.707)\n    np.clip(radii, 0.02, 0.1, out=radii)\n\n    # Adam Optimizer states\n    lr_initial = 0.02\n    beta1 = 0.9\n    beta2 = 0.999\n    eps = 1e-8\n\n    m_c = np.zeros_like(centers)\n    v_c = np.zeros_like(centers)\n    m_r = np.zeros_like(radii)\n    v_r = np.zeros_like(radii)\n\n    max_steps = 12000\n    lam_start = 5.0\n    lam_end = 2e6\n    lam_factor = (lam_end / lam_start) ** (1.0 / max_steps)\n\n    diag_indices = np.arange(N)\n\n    for step in range(max_steps):\n        # Simulated annealing for constraints: penalty weight grows exponentially\n        lam = lam_start * (lam_factor ** step)\n        \n        # Smoothly decaying learning rate, ending very fine to settle micro-adjustments\n        lr = lr_initial * (0.0005 ** (step / max_steps))\n\n        # Break perfect symmetry to escape local maxima\n        if 0 < step < max_steps // 2 and step % 500 == 0:\n            centers += np.random.normal(0, 0.001, size=centers.shape)\n            np.clip(centers, 0.0, 1.0, out=centers)\n\n        # Pairwise differences and fast distance calculation\n        diff = centers[:, :, np.newaxis, :] - centers[:, np.newaxis, :, :]\n        dist = np.sqrt(np.sum(diff * diff, axis=-1))\n        \n        # Ignore self-intersection by setting diagonal distance arbitrarily high\n        dist[:, diag_indices, diag_indices] = 10.0\n\n        # Calculate overlap magnitudes\n        sum_r = radii[:, :, np.newaxis] + radii[:, np.newaxis, :]\n        overlap = np.maximum(0, sum_r - dist)\n\n        # Gradients w.r.t pairwise overlaps\n        grad_r_overlap = 2.0 * np.sum(overlap, axis=2)\n\n        force_mag = 2.0 * overlap / (dist + 1e-8)\n        grad_c_overlap = -np.sum(force_mag[..., np.newaxis] * diff, axis=2)\n\n        # Boundary constraints\n        x = centers[..., 0]\n        y = centers[..., 1]\n        r = radii\n\n        p_L = np.maximum(0, r - x)\n        p_R = np.maximum(0, x + r - 1.0)\n        p_B = np.maximum(0, r - y)\n        p_T = np.maximum(0, y + r - 1.0)\n\n        grad_r_bounds = 2.0 * (p_L + p_R + p_B + p_T)\n        grad_x_bounds = -2.0 * p_L + 2.0 * p_R\n        grad_y_bounds = -2.0 * p_B + 2.0 * p_T\n        grad_c_bounds = np.stack([grad_x_bounds, grad_y_bounds], axis=-1)\n\n        # Combine gradients. Objective is maximizing sum(radii) -> min -sum(r)\n        grad_r = -1.0 + lam * (grad_r_overlap + grad_r_bounds)\n        grad_c = lam * (grad_c_overlap + grad_c_bounds)\n\n        # Clip gradients to prevent numeric explosion\n        np.clip(grad_r, -1e4, 1e4, out=grad_r)\n        np.clip(grad_c, -1e4, 1e4, out=grad_c)\n\n        # Apply in-place Adam updates for computation speed\n        m_c *= beta1\n        m_c += (1.0 - beta1) * grad_c\n        v_c *= beta2\n        v_c += (1.0 - beta2) * (grad_c * grad_c)\n        centers -= lr * m_c / (np.sqrt(v_c) + eps)\n\n        m_r *= beta1\n        m_r += (1.0 - beta1) * grad_r\n        v_r *= beta2\n        v_r += (1.0 - beta2) * (grad_r * grad_r)\n        radii -= lr * m_r / (np.sqrt(v_r) + eps)\n\n        # Enforce valid physical ranges to maintain optimizer stability\n        np.maximum(radii, 0.001, out=radii)\n        np.clip(centers, 0.0, 1.0, out=centers)\n\n    # Final pass: Linear Programming for exact mathematical validity and maximization\n    best_sum = -1.0\n    best_centers = None\n    best_radii = None\n\n    idx_i, idx_j = np.triu_indices(N, 1)\n    A_ub_base = np.zeros((len(idx_i), N))\n    A_ub_base[np.arange(len(idx_i)), idx_i] = 1.0\n    A_ub_base[np.arange(len(idx_i)), idx_j] = 1.0\n\n    # Sort batches and only apply linprog to the top candidates to save compute\n    batch_scores = np.sum(radii, axis=1)\n    top_batches = np.argsort(batch_scores)[-20:][::-1]\n\n    for b in top_batches:\n        c = np.clip(centers[b], 0.0, 1.0)\n        c_obj = -np.ones(N)\n        \n        bounds = []\n        # Bound constraints per circle mapped natively to LP bounds for speed\n        for i in range(N):\n            x_i, y_i = c[i]\n            max_r = max(0.0, float(min(x_i, 1.0 - x_i, y_i, 1.0 - y_i)))\n            bounds.append((0.0, max_r))\n\n        b_ub = np.linalg.norm(c[idx_i] - c[idx_j], axis=1)\n\n        try:\n            res = linprog(c_obj, A_ub=A_ub_base, b_ub=b_ub,\n                          bounds=bounds, method='highs')\n            if res.success:\n                s_r = -res.fun\n                if s_r > best_sum:\n                    best_sum = s_r\n                    best_centers = c.copy()\n                    best_radii = res.x.copy()\n        except Exception:\n            pass\n\n    # Fallback to mathematically rigorous manual trimming if LP somehow fails entirely\n    if best_centers is None:\n        best_idx = int(top_batches[0])\n        best_centers = np.clip(centers[best_idx], 0.0, 1.0)\n        best_radii = np.maximum(radii[best_idx], 0.0)\n        for _ in range(1500):\n            best_radii = np.minimum(best_radii, best_centers[..., 0])\n            best_radii = np.minimum(best_radii, 1.0 - best_centers[..., 0])\n            best_radii = np.minimum(best_radii, best_centers[..., 1])\n            best_radii = np.minimum(best_radii, 1.0 - best_centers[..., 1])\n\n            for i in range(N):\n                for j in range(i + 1, N):\n                    dist_ij = np.linalg.norm(best_centers[i] - best_centers[j])\n                    if best_radii[i] + best_radii[j] > dist_ij:\n                        overlap_ij = best_radii[i] + best_radii[j] - dist_ij\n                        # Reduce each radius symmetrically\n                        best_radii[i] -= overlap_ij * 0.505\n                        best_radii[j] -= overlap_ij * 0.505\n            best_radii = np.maximum(best_radii, 0.0)\n        best_sum = np.sum(best_radii)\n\n    return best_centers, best_radii, float(best_sum)\n\n\n# Create an alias to ensure maximum compatibility with the evaluator hook\nrun_packing = construct_packing\n```\nKey features: Performs well on validity (1.0000), Performs well on sum_radii (2.6309), Performs well on target_ratio (0.9984), Performs well on combined_score (0.9984), Performs well on radius_variance (0.0045), Performs well on spatial_spread (0.1921), Performs well on eval_time (53.9141)\n\n### Program 2 (Score: 0.9982)\n```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\n\ndef construct_packing():\n    \"\"\"\n    Construct an optimized arrangement of 26 circles in a unit square\n    to maximize the sum of their radii.\n\n    Returns:\n        Tuple of (centers, radii, sum_of_radii)\n        centers: np.array of shape (26, 2) with (x, y) coordinates\n        radii: np.array of shape (26) with radius of each circle\n        sum_of_radii: Sum of all radii\n    \"\"\"\n    B = 40  # Batch size for parallel exploration\n    N = 26  # Number of circles\n    np.random.seed(42)\n\n    centers = np.random.uniform(0.1, 0.9, size=(B, N, 2))\n\n    # Pattern 1: Edge/Corner biased (Beta distribution)\n    centers[0:10] = np.random.beta(0.3, 0.3, size=(10, N, 2))\n\n    # Pattern 2: Concentric rings placement\n    for b in range(10, 20):\n        centers[b, 0] = [0.5, 0.5]\n        idx = 1\n        angle_offset_1 = np.random.uniform(0, 2 * np.pi)\n        for i in range(7):\n            theta = 2.0 * np.pi * i / 7.0 + angle_offset_1\n            centers[b, idx] = [0.5 + 0.22 * np.cos(theta),\n                               0.5 + 0.22 * np.sin(theta)]\n            idx += 1\n        angle_offset_2 = np.random.uniform(0, 2 * np.pi)\n        for i in range(18):\n            theta = 2.0 * np.pi * i / 18.0 + angle_offset_2\n            centers[b, idx] = [0.5 + 0.44 * np.cos(theta),\n                               0.5 + 0.44 * np.sin(theta)]\n            idx += 1\n        centers[b] += np.random.normal(0, 0.005, size=(N, 2))\n\n    # Pattern 3: Golden ratio spiral (Fibonacci)\n    phi = (1.0 + np.sqrt(5.0)) / 2.0\n    for b in range(20, 30):\n        angle_offset = np.random.uniform(0, 2 * np.pi)\n        for i in range(N):\n            r_dist = np.sqrt((i + 0.5) / N) * 0.45\n            theta = 2.0 * np.pi * i / phi + angle_offset\n            centers[b, i, 0] = 0.5 + r_dist * np.cos(theta)\n            centers[b, i, 1] = 0.5 + r_dist * np.sin(theta)\n        centers[b] += np.random.normal(0, 0.01, size=(N, 2))\n\n    # Pattern 4: Uniform grid roughly tailored for square packing\n    xv, yv = np.meshgrid(np.linspace(0.1, 0.9, 6), np.linspace(0.1, 0.9, 5))\n    grid_pts = np.stack([xv.flatten(), yv.flatten()], axis=-1)\n    for b in range(30, 40):\n        idx = np.random.choice(len(grid_pts), N, replace=False)\n        centers[b] = grid_pts[idx] + np.random.normal(0, 0.01, size=(N, 2))\n\n    # Keep all centers in a safe bounding box initially\n    centers = np.clip(centers, 0.02, 0.98)\n\n    # Size placement bias: larger radii in the center, smaller in corners/edges\n    dist_to_center = np.linalg.norm(centers - 0.5, axis=-1)\n    radii = 0.08 - 0.05 * (dist_to_center / 0.707)\n    radii = np.clip(radii, 0.02, 0.1)\n\n    # Adam Optimizer states\n    lr_initial = 0.02\n    beta1 = 0.9\n    beta2 = 0.999\n    eps = 1e-8\n\n    m_c = np.zeros_like(centers)\n    v_c = np.zeros_like(centers)\n    m_r = np.zeros_like(radii)\n    v_r = np.zeros_like(radii)\n\n    max_steps = 10000\n    lam_start = 5.0\n    lam_end = 2e6\n    lam_factor = (lam_end / lam_start) ** (1.0 / max_steps)\n\n    mask = np.eye(N)[np.newaxis, :, :]\n\n    for step in range(max_steps):\n        # Simulated annealing for constraints: penalty weight grows exponentially\n        lam = lam_start * (lam_factor ** step)\n        \n        # Smoothly decaying learning rate, ending very fine to settle micro-adjustments\n        lr = lr_initial * (0.0005 ** (step / max_steps))\n\n        # Break perfect symmetry to escape local maxima\n        if step > 0 and step < max_steps // 2 and step % 500 == 0:\n            centers += np.random.normal(0, 0.001, size=centers.shape)\n            centers = np.clip(centers, 0.0, 1.0)\n\n        # Pairwise differences and distances\n        diff = centers[:, :, np.newaxis, :] - centers[:, np.newaxis, :, :]\n        dist = np.linalg.norm(diff, axis=-1)\n\n        # Calculate overlap magnitudes\n        sum_r = radii[:, :, np.newaxis] + radii[:, np.newaxis, :]\n        overlap = np.maximum(0, sum_r - dist)\n        overlap = overlap * (1.0 - mask)  # Exclude self-intersections\n\n        # Gradients w.r.t pairwise overlaps\n        grad_r_overlap = 2.0 * np.sum(overlap, axis=2)\n\n        dist_safe = dist + mask + 1e-8\n        force_mag = 2.0 * overlap / dist_safe\n        grad_c_overlap = -np.sum(force_mag[..., np.newaxis] * diff, axis=2)\n\n        # Boundary constraints\n        x = centers[..., 0]\n        y = centers[..., 1]\n        r = radii\n\n        p_L = np.maximum(0, r - x)\n        p_R = np.maximum(0, x + r - 1.0)\n        p_B = np.maximum(0, r - y)\n        p_T = np.maximum(0, y + r - 1.0)\n\n        grad_r_bounds = 2.0 * (p_L + p_R + p_B + p_T)\n        grad_x_bounds = -2.0 * p_L + 2.0 * p_R\n        grad_y_bounds = -2.0 * p_B + 2.0 * p_T\n        grad_c_bounds = np.stack([grad_x_bounds, grad_y_bounds], axis=-1)\n\n        # Combine gradients. Objective is maximizing sum(radii) -> min -sum(r)\n        grad_r = -1.0 + lam * (grad_r_overlap + grad_r_bounds)\n        grad_c = lam * (grad_c_overlap + grad_c_bounds)\n\n        # Clip gradients to prevent numeric explosion\n        grad_r = np.clip(grad_r, -1e4, 1e4)\n        grad_c = np.clip(grad_c, -1e4, 1e4)\n\n        # Apply Adam updates\n        m_c = beta1 * m_c + (1 - beta1) * grad_c\n        v_c = beta2 * v_c + (1 - beta2) * (grad_c ** 2)\n        centers -= lr * m_c / (np.sqrt(v_c) + eps)\n\n        m_r = beta1 * m_r + (1 - beta1) * grad_r\n        v_r = beta2 * v_r + (1 - beta2) * (grad_r ** 2)\n        radii -= lr * m_r / (np.sqrt(v_r) + eps)\n\n        # Enforce valid physical ranges to maintain optimizer stability\n        radii = np.maximum(radii, 0.001)\n        centers = np.clip(centers, 0.0, 1.0)\n\n    # Final pass: Linear Programming for exact mathematical validity and maximization\n    best_sum = -1.0\n    best_centers = None\n    best_radii = None\n\n    idx_i, idx_j = np.triu_indices(N, 1)\n    A_ub_base = np.zeros((len(idx_i), N))\n    A_ub_base[np.arange(len(idx_i)), idx_i] = 1.0\n    A_ub_base[np.arange(len(idx_i)), idx_j] = 1.0\n\n    for b in range(B):\n        c = np.clip(centers[b], 0.0, 1.0)\n        c_obj = -np.ones(N)\n        \n        bounds = []\n        # Bound constraints per circle mapped natively to LP bounds for speed\n        for i in range(N):\n            x_i, y_i = c[i]\n            max_r = max(0.0, float(min(x_i, 1.0 - x_i, y_i, 1.0 - y_i)))\n            bounds.append((0.0, max_r))\n\n        b_ub = np.linalg.norm(c[idx_i] - c[idx_j], axis=1)\n\n        try:\n            res = linprog(c_obj, A_ub=A_ub_base, b_ub=b_ub,\n                          bounds=bounds, method='highs')\n            if res.success:\n                s_r = -res.fun\n                if s_r > best_sum:\n                    best_sum = s_r\n                    best_centers = c.copy()\n                    best_radii = res.x.copy()\n        except Exception:\n            pass\n\n    # Fallback if LP somehow fails\n    if best_centers is None:\n        best_centers = np.clip(centers[0], 0.0, 1.0)\n        best_radii = np.maximum(radii[0], 0.0)\n\n    # Final rigorous manual trimming to ensure absolute mathematical validity\n    best_radii = np.maximum(best_radii, 0.0)\n    for _ in range(500):\n        best_radii = np.minimum(best_radii, best_centers[:, 0])\n        best_radii = np.minimum(best_radii, 1.0 - best_centers[:, 0])\n        best_radii = np.minimum(best_radii, best_centers[:, 1])\n        best_radii = np.minimum(best_radii, 1.0 - best_centers[:, 1])\n\n        dist_ij = np.linalg.norm(best_centers[idx_i] - best_centers[idx_j], axis=1)\n        sum_r = best_radii[idx_i] + best_radii[idx_j]\n        overlap = np.maximum(0.0, sum_r - dist_ij)\n        \n        if np.max(overlap) < 1e-12:\n            break\n            \n        for k, (i, j) in enumerate(zip(idx_i, idx_j)):\n            if overlap[k] > 0:\n                # Reduce each radius symmetrically by slightly more than half to ensure convergence\n                best_radii[i] -= overlap[k] * 0.505\n                best_radii[j] -= overlap[k] * 0.505\n\n        best_radii = np.maximum(best_radii, 0.0)\n\n    return best_centers, best_radii, float(np.sum(best_radii))\n\n# Create an alias to ensure maximum compatibility with evaluator hooks\nrun_packing = construct_packing\n```\nKey features: Performs well on validity (1.0000), Performs well on sum_radii (2.6303), Performs well on target_ratio (0.9982), Performs well on combined_score (0.9982), Performs well on radius_variance (0.0049), Performs well on spatial_spread (0.1959), Performs well on eval_time (19.3155)\n\n### Program 3 (Score: 0.9982)\n```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\n\ndef run_packing():\n    \"\"\"\n    Construct an optimized arrangement of 26 circles in a unit square\n    to maximize the sum of their radii.\n\n    Returns:\n        Tuple of (centers, radii, sum_of_radii)\n        centers: np.array of shape (26, 2) with (x, y) coordinates\n        radii: np.array of shape (26) with radius of each circle\n        sum_of_radii: Sum of all radii\n    \"\"\"\n    np.random.seed(42)\n    B = 64  # Batch size for parallel exploration\n    N = 26  # Number of circles\n\n    # Initialization\n    centers = np.random.uniform(0.1, 0.9, size=(B, N, 2))\n\n    # Pattern 1: Edge/Corner biased (Beta distribution)\n    centers[0:16] = np.random.beta(0.3, 0.3, size=(16, N, 2))\n\n    # Pattern 2: Hexagonal grid roughly tailored for square packing\n    hex_centers = []\n    for row in range(7):\n        for col in range(7):\n            x = 0.05 + col * 0.15 + (0.075 if row % 2 == 1 else 0.0)\n            y = 0.05 + row * 0.15\n            if x <= 0.95 and y <= 0.95:\n                hex_centers.append([x, y])\n    hex_pts = np.array(hex_centers)\n    for b in range(16, 24):\n        idx = np.random.choice(len(hex_pts), N, replace=False)\n        centers[b] = hex_pts[idx] + np.random.normal(0, 0.01, size=(N, 2))\n\n    # Pattern 3: Standard uniform grid\n    xv, yv = np.meshgrid(np.linspace(0.08, 0.92, 6), np.linspace(0.08, 0.92, 6))\n    grid_pts = np.stack([xv.flatten(), yv.flatten()], axis=-1)\n    for b in range(24, 32):\n        idx = np.random.choice(len(grid_pts), N, replace=False)\n        centers[b] = grid_pts[idx] + np.random.normal(0, 0.01, size=(N, 2))\n\n    # Pattern 4: Concentric rings placement\n    for b in range(32, 48):\n        centers[b, 0] = [0.5, 0.5]\n        idx = 1\n        angle_offset_1 = np.random.uniform(0, 2 * np.pi)\n        for i in range(7):\n            theta = 2.0 * np.pi * i / 7.0 + angle_offset_1\n            centers[b, idx] = [0.5 + 0.22 * np.cos(theta),\n                               0.5 + 0.22 * np.sin(theta)]\n            idx += 1\n        angle_offset_2 = np.random.uniform(0, 2 * np.pi)\n        for i in range(18):\n            theta = 2.0 * np.pi * i / 18.0 + angle_offset_2\n            centers[b, idx] = [0.5 + 0.44 * np.cos(theta),\n                               0.5 + 0.44 * np.sin(theta)]\n            idx += 1\n        centers[b] += np.random.normal(0, 0.005, size=(N, 2))\n\n    # Pattern 5: Golden ratio spiral (Fibonacci)\n    phi = (1.0 + np.sqrt(5.0)) / 2.0\n    for b in range(48, 56):\n        angle_offset = np.random.uniform(0, 2 * np.pi)\n        for i in range(N):\n            r_dist = np.sqrt((i + 0.5) / N) * 0.45\n            theta = 2.0 * np.pi * i / phi + angle_offset\n            centers[b, i, 0] = 0.5 + r_dist * np.cos(theta)\n            centers[b, i, 1] = 0.5 + r_dist * np.sin(theta)\n        centers[b] += np.random.normal(0, 0.01, size=(N, 2))\n\n    # Pattern 6: Random uniform with noise\n    centers[56:64] = np.random.uniform(0.05, 0.95, size=(8, N, 2))\n\n    # Keep all centers in a safe bounding box initially\n    centers = np.clip(centers, 0.02, 0.98)\n\n    # Size placement bias: larger radii in the center, smaller in corners/edges\n    dist_to_center = np.linalg.norm(centers - 0.5, axis=-1)\n    radii = 0.08 - 0.05 * (dist_to_center / 0.707)\n    radii = np.clip(radii, 0.02, 0.1)\n\n    # Adam Optimizer states\n    lr_initial = 0.02\n    beta1 = 0.9\n    beta2 = 0.999\n    eps = 1e-8\n\n    m_c = np.zeros_like(centers)\n    v_c = np.zeros_like(centers)\n    m_r = np.zeros_like(radii)\n    v_r = np.zeros_like(radii)\n\n    max_steps = 12000\n    lam_start = 5.0\n    lam_end = 2e6\n    lam_factor = (lam_end / lam_start) ** (1.0 / max_steps)\n\n    mask = np.eye(N)[np.newaxis, :, :]\n\n    for step in range(max_steps):\n        # Simulated annealing for constraints: penalty weight grows exponentially\n        lam = lam_start * (lam_factor ** step)\n        \n        # Smoothly decaying learning rate, ending very fine to settle micro-adjustments\n        lr = lr_initial * (0.0005 ** (step / max_steps))\n\n        # Break perfect symmetry to escape local maxima\n        if step > 0 and step < max_steps // 2 and step % 500 == 0:\n            centers += np.random.normal(0, 0.001, size=centers.shape)\n            centers = np.clip(centers, 0.0, 1.0)\n\n        # Pairwise differences and distances\n        diff = centers[:, :, np.newaxis, :] - centers[:, np.newaxis, :, :]\n        dist = np.linalg.norm(diff, axis=-1)\n\n        # Calculate overlap magnitudes\n        sum_r = radii[:, :, np.newaxis] + radii[:, np.newaxis, :]\n        overlap = np.maximum(0, sum_r - dist)\n        overlap = overlap * (1.0 - mask)  # Exclude self-intersections\n\n        # Gradients w.r.t pairwise overlaps\n        grad_r_overlap = 2.0 * np.sum(overlap, axis=2)\n\n        dist_safe = dist + mask + 1e-8\n        force_mag = 2.0 * overlap / dist_safe\n        grad_c_overlap = -np.sum(force_mag[..., np.newaxis] * diff, axis=2)\n\n        # Boundary constraints\n        x = centers[..., 0]\n        y = centers[..., 1]\n        r = radii\n\n        p_L = np.maximum(0, r - x)\n        p_R = np.maximum(0, x + r - 1.0)\n        p_B = np.maximum(0, r - y)\n        p_T = np.maximum(0, y + r - 1.0)\n\n        grad_r_bounds = 2.0 * (p_L + p_R + p_B + p_T)\n        grad_x_bounds = -2.0 * p_L + 2.0 * p_R\n        grad_y_bounds = -2.0 * p_B + 2.0 * p_T\n        grad_c_bounds = np.stack([grad_x_bounds, grad_y_bounds], axis=-1)\n\n        # Combine gradients. Objective is maximizing sum(radii) -> min -sum(r)\n        grad_r = -1.0 + lam * (grad_r_overlap + grad_r_bounds)\n        grad_c = lam * (grad_c_overlap + grad_c_bounds)\n\n        # Clip gradients to prevent numeric explosion\n        grad_r = np.clip(grad_r, -1e4, 1e4)\n        grad_c = np.clip(grad_c, -1e4, 1e4)\n\n        # Apply Adam updates\n        m_c = beta1 * m_c + (1 - beta1) * grad_c\n        v_c = beta2 * v_c + (1 - beta2) * (grad_c ** 2)\n        centers -= lr * m_c / (np.sqrt(v_c) + eps)\n\n        m_r = beta1 * m_r + (1 - beta1) * grad_r\n        v_r = beta2 * v_r + (1 - beta2) * (grad_r ** 2)\n        radii -= lr * m_r / (np.sqrt(v_r) + eps)\n\n        # Enforce valid physical ranges to maintain optimizer stability\n        radii = np.maximum(radii, 0.001)\n        centers = np.clip(centers, 0.0, 1.0)\n\n    # Final pass: Linear Programming for exact mathematical validity and maximization\n    best_sum = -1.0\n    best_centers = None\n    best_radii = None\n\n    for b in range(B):\n        c = np.clip(centers[b], 0.0, 1.0)\n        c_obj = -np.ones(N)\n        A_ub = []\n        b_ub = []\n        bounds = []\n\n        # Bound constraints per circle mapped natively to LP bounds for speed\n        for i in range(N):\n            x_i, y_i = c[i]\n            max_r = max(0.0, float(min(x_i, 1.0 - x_i, y_i, 1.0 - y_i)))\n            bounds.append((0.0, max_r))\n\n        # Pairwise distance constraints to ensure no circle overlaps\n        for i in range(N):\n            for j in range(i + 1, N):\n                dist_ij = np.linalg.norm(c[i] - c[j])\n                row = np.zeros(N)\n                row[i] = 1.0\n                row[j] = 1.0\n                A_ub.append(row)\n                b_ub.append(dist_ij)\n\n        A_ub = np.array(A_ub)\n        b_ub = np.array(b_ub)\n\n        try:\n            res = linprog(c_obj, A_ub=A_ub, b_ub=b_ub,\n                          bounds=bounds, method='highs')\n            if res.success:\n                s_r = -res.fun\n                if s_r > best_sum:\n                    best_sum = s_r\n                    best_centers = c\n                    best_radii = res.x\n        except Exception:\n            pass\n\n    # Fallback to mathematically rigorous manual trimming if LP somehow fails\n    if best_centers is None:\n        best_centers = np.clip(centers[0], 0.0, 1.0)\n        best_radii = np.maximum(radii[0], 0.0)\n        for _ in range(1500):\n            best_radii = np.minimum(best_radii, best_centers[..., 0])\n            best_radii = np.minimum(best_radii, 1.0 - best_centers[..., 0])\n            best_radii = np.minimum(best_radii, best_centers[..., 1])\n            best_radii = np.minimum(best_radii, 1.0 - best_centers[..., 1])\n\n            for i in range(N):\n                for j in range(i + 1, N):\n                    dist_ij = np.linalg.norm(best_centers[i] - best_centers[j])\n                    if best_radii[i] + best_radii[j] > dist_ij:\n                        overlap_ij = best_radii[i] + best_radii[j] - dist_ij\n                        # Reduce each radius symmetrically\n                        best_radii[i] -= overlap_ij * 0.505\n                        best_radii[j] -= overlap_ij * 0.505\n            best_radii = np.maximum(best_radii, 0.0)\n        best_sum = np.sum(best_radii)\n\n    return best_centers, best_radii, float(best_sum)\n\n\n# Create an alias to ensure maximum compatibility with the evaluator hook\nconstruct_packing = run_packing\n```\nKey features: Performs well on validity (1.0000), Performs well on sum_radii (2.6302), Performs well on target_ratio (0.9982), Performs well on combined_score (0.9982), Performs well on radius_variance (0.0045), Performs well on spatial_spread (0.1879), Performs well on eval_time (37.1331)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3333)\n```python\nimport numpy as np\n\n\ndef run_packing():\n    \"\"\"\n    Construct an optimized arrangement of 26 circles in a unit square\n    to maximize the sum of their radii.\n\n    Returns:\n        Tuple of (centers, radii, sum_of_radii)\n        centers: np.array of shape (26, 2) with (x, y) coordinates\n        radii: np.array of shape (26) with radius of each circle\n        sum_of_radii: Sum of all radii\n    \"\"\"\n    np.random.seed(42)\n    B = 150  # Batch size for parallel exploration\n    N = 26  # Number of circles\n    \n    centers = np.random.uniform(0.1, 0.9, size=(B, N, 2))\n\n    # 1. Edge/Corner biased initialization (Beta distribution)\n    centers[0:30] = np.random.beta(0.2, 0.2, size=(30, N, 2))\n    centers[30:60] = np.random.beta(0.3, 0.3, size=(30, N, 2))\n    centers[60:90] = np.random.beta(0.4, 0.4, size=(30, N, 2))\n\n    # 2. Grid-based initialization with noise\n    grid_size = 6\n    xv, yv = np.meshgrid(\n        np.linspace(0.1, 0.9, grid_size),\n        np.linspace(0.1, 0.9, grid_size)\n    )\n    grid_pts = np.stack([xv.flatten(), yv.flatten()], axis=-1)\n    for b in range(90, 110):\n        idx = np.random.choice(len(grid_pts), N, replace=False)\n        centers[b] = grid_pts[idx] + np.random.normal(0, 0.01, size=(N, 2))\n\n    # 3. Concentric rings initialization\n    for b in range(110, 130):\n        centers[b, 0] = [0.5, 0.5]\n        idx = 1\n        for ring_idx, count in enumerate([8, 17]):\n            ring_radius = 0.22 if ring_idx == 0 else 0.45\n            angle_offset = np.random.uniform(0, 2 * np.pi)\n            for i in range(count):\n                if idx < N:\n                    angle = 2.0 * np.pi * i / count + angle_offset\n                    centers[b, idx] = [\n                        0.5 + ring_radius * np.cos(angle),\n                        0.5 + ring_radius * np.sin(angle)\n                    ]\n                    idx += 1\n        centers[b] += np.random.normal(0, 0.01, size=(N, 2))\n\n    # 4. Golden spiral initialization\n    for b in range(130, 150):\n        phi = (1.0 + np.sqrt(5.0)) / 2.0\n        angle_offset = np.random.uniform(0, 2 * np.pi)\n        for i in range(N):\n            r_dist = np.sqrt((i + 0.5) / N) * 0.45 \n            theta = 2.0 * np.pi * i / phi + angle_offset\n            centers[b, i, 0] = 0.5 + r_dist * np.cos(theta)\n            centers[b, i, 1] = 0.5 + r_dist * np.sin(theta)\n        centers[b] += np.random.normal(0, 0.01, size=(N, 2))\n\n    # Ensure everything remains in valid approximate bounds\n    centers = np.clip(centers, 0.02, 0.98)\n\n    # Initialize larger radii for central circles, smaller for corner/edge\n    dist_to_center = np.linalg.norm(centers - 0.5, axis=-1)\n    radii = 0.08 - 0.05 * (dist_to_center / 0.707)\n    radii = np.clip(radii, 0.02, 0.1)\n\n    # Adam Optimizer states\n    lr_initial = 0.02\n    beta1 = 0.9\n    beta2 = 0.999\n    eps = 1e-8\n\n    m_c = np.zeros_like(centers)\n    v_c = np.zeros_like(centers)\n    m_r = np.zeros_like(radii)\n    v_r = np.zeros_like(radii)\n\n    max_steps = 20000\n    lam_start = 5.0\n    lam_end = 5e6\n    lam_factor = (lam_end / lam_start) ** (1.0 / max_steps)\n\n    mask = np.eye(N)[np.newaxis, :, :]\n\n    for step in range(max_steps):\n        # Exponentially growing lambda for constraints (simulated annealing)\n        lam = lam_start * (lam_factor ** step)\n        \n        # Smoothly decaying learning rate\n        lr = lr_initial * (0.01 ** (step / max_steps))\n\n        # Break perfect symmetry to escape local maxima\n        if step < max_steps // 2 and step % 500 == 0:\n            centers += np.random.normal(0, 0.001, size=centers.shape)\n            centers = np.clip(centers, 0.0, 1.0)\n\n        # Pairwise differences and distances\n        diff = centers[:, :, np.newaxis, :] - centers[:, np.newaxis, :, :]\n        dist = np.linalg.norm(diff, axis=-1)\n\n        # Calculate overlap magnitudes\n        sum_r = radii[:, :, np.newaxis] + radii[:, np.newaxis, :]\n        overlap = np.maximum(0, sum_r - dist)\n        overlap = overlap * (1.0 - mask)  # Exclude self-intersections\n\n        # Gradients with respect to pairwise overlaps\n        grad_r_overlap = 2.0 * np.sum(overlap, axis=2)\n\n        dist_safe = dist + mask + 1e-8\n        force_mag = 2.0 * overlap / dist_safe\n        grad_c_overlap = -np.sum(force_mag[..., np.newaxis] * diff, axis=2)\n\n        # Boundary constraints\n        x = centers[..., 0]\n        y = centers[..., 1]\n        r = radii\n\n        p_L = np.maximum(0, r - x)\n        p_R = np.maximum(0, x + r - 1.0)\n        p_B = np.maximum(0, r - y)\n        p_T = np.maximum(0, y + r - 1.0)\n\n        grad_r_bounds = 2.0 * (p_L + p_R + p_B + p_T)\n        grad_x_bounds = -2.0 * p_L + 2.0 * p_R\n        grad_y_bounds = -2.0 * p_B + 2.0 * p_T\n        grad_c_bounds = np.stack([grad_x_bounds, grad_y_bounds], axis=-1)\n\n        # Combine gradients. Objective is maximizing sum(radii) -> min -sum(r)\n        grad_r = -1.0 + lam * (grad_r_overlap + grad_r_bounds)\n        grad_c = lam * (grad_c_overlap + grad_c_bounds)\n\n        # Clip gradients to prevent numeric explosion\n        grad_r = np.clip(grad_r, -1e4, 1e4)\n        grad_c = np.clip(grad_c, -1e4, 1e4)\n\n        # Apply Adam updates\n        m_c = beta1 * m_c + (1 - beta1) * grad_c\n        v_c = beta2 * v_c + (1 - beta2) * (grad_c ** 2)\n        centers -= lr * m_c / (np.sqrt(v_c) + eps)\n\n        m_r = beta1 * m_r + (1 - beta1) * grad_r\n        v_r = beta2 * v_r + (1 - beta2) * (grad_r ** 2)\n        radii -= lr * m_r / (np.sqrt(v_r) + eps)\n\n        # Enforce valid ranges\n        radii = np.maximum(radii, 0.001)\n        centers = np.clip(centers, 0.0, 1.0)\n\n    # Final strict cleanup pass: mathematically eliminate any residual overlaps\n    # Precompute distances since centers are locked\n    diff = centers[:, :, np.newaxis, :] - centers[:, np.newaxis, :, :]\n    dist = np.linalg.norm(diff, axis=-1)\n    \n    for _ in range(10000):\n        # Trim boundaries strictly\n        radii = np.minimum(radii, centers[..., 0])\n        radii = np.minimum(radii, 1.0 - centers[..., 0])\n        radii = np.minimum(radii, centers[..., 1])\n        radii = np.minimum(radii, 1.0 - centers[..., 1])\n\n        # Trim pairwise overlaps\n        sum_r = radii[:, :, np.newaxis] + radii[:, np.newaxis, :]\n        overlap = np.maximum(0, sum_r - dist)\n        overlap = overlap * (1.0 - mask)\n        \n        max_over = np.max(overlap, axis=2)\n        if np.max(max_over) < 1e-11:\n            break\n            \n        # Shrink by slightly more than half of the worst overlap to guarantee convergence\n        radii -= max_over * 0.505\n        radii = np.maximum(radii, 0.0)\n\n    # Return the best batch result\n    sum_radii = np.sum(radii, axis=1)\n    best_idx = int(np.argmax(sum_radii))\n\n    return centers[best_idx], radii[best_idx], float(sum_radii[best_idx])\n```\nKey features: Alternative approach to stage1_passed, Alternative approach to error\n\n### Program D2 (Score: 0.3333)\n```python\nimport numpy as np\n\n\ndef run_packing():\n    \"\"\"\n    Construct an optimized arrangement of 26 circles in a unit square\n    to maximize the sum of their radii.\n\n    Returns:\n        Tuple of (centers, radii, sum_of_radii)\n        centers: np.array of shape (26, 2) with (x, y) coordinates\n        radii: np.array of shape (26) with radius of each circle\n        sum_of_radii: Sum of all radii\n    \"\"\"\n    np.random.seed(42)\n    B = 200  # Batch size for parallel exploration\n    N = 26  # Number of circles\n    \n    centers = np.random.uniform(0.1, 0.9, size=(B, N, 2))\n\n    # 1. Edge/Corner biased initialization (Beta distribution)\n    centers[:15] = np.random.beta(0.2, 0.2, size=(15, N, 2))\n    centers[15:30] = np.random.beta(0.4, 0.4, size=(15, N, 2))\n    centers[30:45] = np.random.beta(0.5, 0.5, size=(15, N, 2))\n\n    # 2. Grid-based initialization with noise\n    grid_size = 6\n    xv, yv = np.meshgrid(np.linspace(0.1, 0.9, grid_size), \n                         np.linspace(0.1, 0.9, grid_size))\n    grid_pts = np.stack([xv.flatten(), yv.flatten()], axis=-1)\n    for b in range(45, 80):\n        idx = np.random.choice(len(grid_pts), N, replace=False)\n        centers[b] = grid_pts[idx] + np.random.normal(0, 0.01, size=(N, 2))\n\n    # 3. Concentric rings initialization\n    for b in range(80, 110):\n        centers[b, 0] = [0.5, 0.5]\n        idx = 1\n        for ring_idx, count in enumerate([8, 17]):\n            ring_radius = 0.22 if ring_idx == 0 else 0.45\n            angle_offset = np.random.uniform(0, 2 * np.pi)\n            for i in range(count):\n                if idx < N:\n                    angle = 2.0 * np.pi * i / count + angle_offset\n                    centers[b, idx] = [0.5 + ring_radius * np.cos(angle), \n                                       0.5 + ring_radius * np.sin(angle)]\n                    idx += 1\n        centers[b] += np.random.normal(0, 0.01, size=(N, 2))\n\n    # 4. Golden spiral initialization\n    for b in range(110, 150):\n        phi = (1.0 + np.sqrt(5.0)) / 2.0\n        angle_offset = np.random.uniform(0, 2 * np.pi)\n        for i in range(N):\n            r_dist = np.sqrt((i + 0.5) / N) * 0.45 \n            theta = 2.0 * np.pi * i / phi + angle_offset\n            centers[b, i, 0] = 0.5 + r_dist * np.cos(theta)\n            centers[b, i, 1] = 0.5 + r_dist * np.sin(theta)\n        centers[b] += np.random.normal(0, 0.01, size=(N, 2))\n\n    # 5. Random\n    centers[150:] = np.random.uniform(0.05, 0.95, size=(B - 150, N, 2))\n\n    # Ensure everything remains in valid approximate bounds\n    centers = np.clip(centers, 0.02, 0.98)\n\n    # Initialize larger radii for central circles, smaller for corner/edge\n    dist_to_center = np.linalg.norm(centers - 0.5, axis=-1)\n    radii = 0.08 - 0.05 * (dist_to_center / 0.707)\n    radii = np.clip(radii, 0.02, 0.1)\n\n    # Adam Optimizer states\n    lr_initial = 0.015\n    beta1 = 0.9\n    beta2 = 0.999\n    eps = 1e-8\n\n    m_c = np.zeros_like(centers)\n    v_c = np.zeros_like(centers)\n    m_r = np.zeros_like(radii)\n    v_r = np.zeros_like(radii)\n\n    max_steps = 25000\n    lam_start = 10.0\n    lam_end = 1e7\n    lam_factor = (lam_end / lam_start) ** (1.0 / max_steps)\n\n    mask = np.eye(N)[np.newaxis, :, :]\n\n    for step in range(max_steps):\n        # Exponentially growing lambda for constraints\n        lam = lam_start * (lam_factor ** step)\n        \n        # Smoothly decaying learning rate\n        lr = lr_initial * (0.01 ** (step / max_steps))\n\n        # Break perfect symmetry to escape local maxima\n        if step < max_steps // 2 and step % 500 == 0:\n            centers += np.random.normal(0, 0.001, size=centers.shape)\n            centers = np.clip(centers, 0.0, 1.0)\n\n        # Pairwise differences and distances\n        diff = centers[:, :, np.newaxis, :] - centers[:, np.newaxis, :, :]\n        dist = np.linalg.norm(diff, axis=-1)\n\n        # Calculate overlap magnitudes\n        sum_r = radii[:, :, np.newaxis] + radii[:, np.newaxis, :]\n        overlap = np.maximum(0, sum_r - dist)\n        overlap = overlap * (1.0 - mask)  # Exclude self-intersections\n\n        # Gradients with respect to pairwise overlaps\n        grad_r_overlap = 2.0 * np.sum(overlap, axis=2)\n\n        dist_safe = dist + mask + 1e-8\n        force_mag = 2.0 * overlap / dist_safe\n        grad_c_overlap = -np.sum(force_mag[..., np.newaxis] * diff, axis=2)\n\n        # Boundary constraints\n        x = centers[..., 0]\n        y = centers[..., 1]\n        r = radii\n\n        p_L = np.maximum(0, r - x)\n        p_R = np.maximum(0, x + r - 1.0)\n        p_B = np.maximum(0, r - y)\n        p_T = np.maximum(0, y + r - 1.0)\n\n        grad_r_bounds = 2.0 * (p_L + p_R + p_B + p_T)\n        grad_x_bounds = -2.0 * p_L + 2.0 * p_R\n        grad_y_bounds = -2.0 * p_B + 2.0 * p_T\n        grad_c_bounds = np.stack([grad_x_bounds, grad_y_bounds], axis=-1)\n\n        # Combine gradients. Objective is maximizing sum(radii) -> min -sum(r)\n        grad_r = -1.0 + lam * (grad_r_overlap + grad_r_bounds)\n        grad_c = lam * (grad_c_overlap + grad_c_bounds)\n\n        # Clip gradients to prevent numeric explosion\n        grad_r = np.clip(grad_r, -1e4, 1e4)\n        grad_c = np.clip(grad_c, -1e4, 1e4)\n\n        # Apply Adam updates\n        m_c = beta1 * m_c + (1 - beta1) * grad_c\n        v_c = beta2 * v_c + (1 - beta2) * (grad_c ** 2)\n        centers -= lr * m_c / (np.sqrt(v_c) + eps)\n\n        m_r = beta1 * m_r + (1 - beta1) * grad_r\n        v_r = beta2 * v_r + (1 - beta2) * (grad_r ** 2)\n        radii -= lr * m_r / (np.sqrt(v_r) + eps)\n\n        # Enforce valid ranges\n        radii = np.maximum(radii, 0.001)\n        centers = np.clip(centers, 0.0, 1.0)\n\n    # Final strict cleanup pass: mathematically eliminate any residual overlaps\n    # Precompute distances since centers are locked\n    diff = centers[:, :, np.newaxis, :] - centers[:, np.newaxis, :, :]\n    dist = np.linalg.norm(diff, axis=-1)\n    \n    for _ in range(10000):\n        # Trim boundaries strictly\n        radii = np.minimum(radii, centers[..., 0])\n        radii = np.minimum(radii, 1.0 - centers[..., 0])\n        radii = np.minimum(radii, centers[..., 1])\n        radii = np.minimum(radii, 1.0 - centers[..., 1])\n\n        # Trim pairwise overlaps\n        sum_r = radii[:, :, np.newaxis] + radii[:, np.newaxis, :]\n        overlap = np.maximum(0, sum_r - dist)\n        overlap = overlap * (1.0 - mask)\n        \n        max_over = np.max(overlap, axis=2)\n        if np.max(max_over) < 1e-11:\n            break\n            \n        # Shrink by slightly more than half of the worst overlap to guarantee convergence\n        radii -= max_over * 0.505\n        radii = np.maximum(radii, 0.0)\n\n    # Return the best batch result\n    sum_radii = np.sum(radii, axis=1)\n    best_idx = int(np.argmax(sum_radii))\n\n    return centers[best_idx], radii[best_idx], float(sum_radii[best_idx])\n```\nKey features: Alternative approach to stage1_passed, Alternative approach to error\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0000, Type: Exploratory)\n```python\n```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\n\ndef evaluate_lps(centers):\n    \"\"\"\n    Evaluates the exact optimal radii for a given set of centers using Linear Programming.\n    Returns the sum of radii and the radii arrays for each batch element.\n    \"\"\"\n    B, N, _ = centers.shape\n    sums = np.zeros(B)\n    all_radii = np.zeros((B, N))\n    \n    idx_i, idx_j = np.triu_indices(N, 1)\n    num_pairs = len(idx_i)\n    \n    A_ub = np.zeros((num_pairs, N))\n    A_ub[np.arange(num_pairs), idx_i] = 1.0\n    A_ub[np.arange(num_pairs), idx_j] = 1.0\n    \n    c_obj = -np.ones(N)\n    \n    for b in range(B):\n        c = np.clip(centers[b], 0.0, 1.0)\n        \n        bounds = []\n        for i in range(N):\n            x_i, y_i = c[i]\n            max_r = max(0.0, float(min(x_i, 1.0 - x_i, y_i, 1.0 - y_i)))\n            bounds.append((0.0, max_r))\n            \n        dist = np.linalg.norm(c[idx_i] - c[idx_j], axis=1)\n        \n        try:\n            res = linprog(c_obj, A_ub=A_ub, b_ub=dist, bounds=bounds, method='highs')\n            if res.success:\n                sums[b] = -res.fun\n                all_radii[b] = res.x\n            else:\n                sums[b] = -1.0\n        except Exception:\n            sums[b] = -1.0\n            \n    return sums, all_radii\n\n\ndef run_adam(centers, radii, max_steps, lr_init, lr_final, lam_start, lam_end, perturb_freq=500, perturb_scale=0.001):\n    \"\"\"\n    Physics-based optimization mimicking gradient descent on a constrained system.\n    Dynamically balances between maximizing radii and zeroing out constraint violations.\n    \"\"\"\n    B, N, _ = centers.shape\n    m_c = np.zeros_like(centers)\n    v_c = np.zeros_like(centers)\n    m_r = np.zeros_like(radii)\n    v_r = np.zeros_like(radii)\n\n    beta1 = 0.9\n    beta2 = 0.999\n    eps = 1e-8\n\n    lam_factor = (lam_end / lam_start) ** (1.0 / max_steps) if max_steps > 0 else 1.0\n    lr_factor = (lr_final / lr_init) ** (1.0 / max_steps) if max_steps > 0 else 1.0\n\n    mask = np.eye(N)[np.newaxis, :, :]\n\n    for step in range(max_steps):\n        lam = lam_start * (lam_factor ** step)\n        lr = lr_init * (lr_factor ** step)\n\n        # Adam Bias Correction for stable initialization of\n```\nUnique approach: Modification:, [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.0000, Type: Exploratory)\n```python\n\"\"\"Iterative physics-based circle packing optimization for n=26 circles.\"\"\"\nimport numpy as np\n\n\ndef construct_packing():\n    \"\"\"\n    Construct an optimized arrangement of 26 circles in a unit square\n    to maximize the sum of their radii.\n\n    Returns:\n        Tuple of (centers, radii, sum_of_radii)\n        centers: np.array of shape (26, 2) with (x, y) coordinates\n        radii: np.array of shape (26) with radius of each circle\n        sum_of_radii: Sum of all radii\n    \"\"\"\n    np.random.seed(42)\n    b_size = 32  # Batch size for parallel exploration\n    n_circles = 26  # Number of circles\n    \n    centers = np.random.uniform(0.1, 0.9, size=(b_size, n_circles, 2))\n\n    # 1. Edge/Corner biased initialization (Beta distribution)\n    centers[:8] = np.random.beta(0.3, 0.3, size=(8, n_circles, 2))\n\n    # 2. Grid-based initialization with noise\n    grid_size = 6\n    xv, yv = np.meshgrid(np.linspace(0.1, 0.9, grid_size), \n                         np.linspace(0.1, 0.9, grid_size))\n    grid_pts = np.stack([xv.flatten(), yv.flatten()], axis=-1)\n    for b in range(8, 16):\n        idx = np.random.choice(len(grid_pts), n_circles, replace=False)\n        centers[b] = grid_pts[idx] + np.random.normal(0, 0.01, size=(n_circles, 2))\n\n    # 3. Concentric rings initialization\n    for b in range(16, 24):\n        centers[b, 0] = [0.5, 0.5]\n        idx = 1\n        for ring_idx, count in enumerate([8, 17]):\n            ring_radius = 0.22 if ring_idx == 0 else 0.45\n            angle_offset = np.random.uniform(0, 2 * np.pi)\n            for i in range(count):\n                if idx < n_circles:\n                    angle = 2.0 * np.pi * i / count + angle_offset\n                    centers[b, idx] = [0.5 + ring_radius * np.cos(angle), \n                                       0.5 + ring_radius * np.sin(angle)]\n                    idx += 1\n        centers[b] += np.random.normal(0, 0.01, size=(n_circles, 2))\n\n    # 4. Golden spiral initialization\n    for b in range(24, b_size):\n        phi = (1.0 + np.sqrt(5.0)) / 2.0\n        angle_offset = np.random.uniform(0, 2 * np.pi)\n        for i in range(n_circles):\n            r_dist = np.sqrt((i + 0.5) / n_circles) * 0.45 \n            theta = 2.0 * np.pi * i / phi + angle_offset\n            centers[b, i, 0] = 0.5 + r_dist * np.cos(theta)\n            centers[b, i, 1] = 0.5 + r_dist * np.sin(theta)\n        centers[b] += np.random.normal(0, 0.01, size=(n_circles, 2))\n\n    # Ensure everything remains in valid approximate bounds\n    centers = np.clip(centers, 0.02, 0.98)\n\n    # Initialize larger radii for central circles, smaller for corner/edge\n    dist_to_center = np.linalg.norm(centers - 0.5, axis=-1)\n    radii = 0.08 - 0.05 * (dist_to_center / 0.707)\n    radii = np.clip(radii, 0.02, 0.1)\n\n    # Adam Optimizer states\n    lr_initial = 0.015\n    beta1 = 0.9\n    beta2 = 0.999\n    eps = 1e-8\n\n    m_c = np.zeros_like(centers)\n    v_c = np.zeros_like(centers)\n    m_r = np.zeros_like(radii)\n    v_r = np.zeros_like(radii)\n\n    max_steps = 10000\n    lam_start = 10.0\n    lam_end = 2e6\n    lam_factor = (lam_end / lam_start) ** (1.0 / max_steps)\n\n    mask = np.eye(n_circles)[np.newaxis, :, :]\n\n    for step in range(max_steps):\n        # Exponentially growing lambda for constraints\n        lam = lam_start * (lam_factor ** step)\n        \n        # Smoothly decaying learning rate\n        lr = lr_initial * (0.01 ** (step / max_steps))\n\n        # Break perfect symmetry to escape local maxima\n        if step < max_steps // 2 and step % 400 == 0:\n            centers += np.random.normal(0, 0.001, size=centers.shape)\n            centers = np.clip(centers, 0.0, 1.0)\n\n        # Pairwise differences and distances\n        diff = centers[:, :, np.newaxis, :] - centers[:, np.newaxis, :, :]\n        dist = np.linalg.norm(diff, axis=-1)\n\n        # Calculate overlap magnitudes\n        sum_r = radii[:, :, np.newaxis] + radii[:, np.newaxis, :]\n        overlap = np.maximum(0, sum_r - dist)\n        overlap = overlap * (1.0 - mask)  # Exclude self-intersections\n\n        # Gradients with respect to pairwise overlaps\n        grad_r_overlap = 2.0 * np.sum(overlap, axis=2)\n\n        dist_safe = dist + mask + 1e-8\n        force_mag = 2.0 * overlap / dist_safe\n        grad_c_overlap = -np.sum(force_mag[..., np.newaxis] * diff, axis=2)\n\n        # Boundary constraints\n        x = centers[..., 0]\n        y = centers[..., 1]\n        r = radii\n\n        p_l = np.maximum(0, r - x)\n        p_r = np.maximum(0, x + r - 1.0)\n        p_b = np.maximum(0, r - y)\n        p_t = np.maximum(0, y + r - 1.0)\n\n        grad_r_bounds = 2.0 * (p_l + p_r + p_b + p_t)\n        grad_x_bounds = -2.0 * p_l + 2.0 * p_r\n        grad_y_bounds = -2.0 * p_b + 2.0 * p_t\n        grad_c_bounds = np.stack([grad_x_bounds, grad_y_bounds], axis=-1)\n\n        # Combine gradients. Objective is maximizing sum(radii) -> min -sum(r)\n        grad_r = -1.0 + lam * (grad_r_overlap + grad_r_bounds)\n        grad_c = lam * (grad_c_overlap + grad_c_bounds)\n\n        # Apply Adam updates\n        m_c = beta1 * m_c + (1.0 - beta1) * grad_c\n        v_c = beta2 * v_c + (1.0 - beta2) * (grad_c ** 2)\n        centers -= lr * m_c / (np.sqrt(v_c) + eps)\n\n        m_r = beta1 * m_r + (1.0 - beta1) * grad_r\n        v_r = beta2 * v_r + (1.0 - beta2) * (grad_r ** 2)\n        radii -= lr * m_r / (np.sqrt(v_r) + eps)\n\n        # Enforce valid ranges\n        radii = np.maximum(radii, 0.001)\n        centers = np.clip(centers, 0.0, 1.0)\n\n    # Final strict cleanup pass: mathematically eliminate any residual overlaps\n    # Precompute distances since centers are locked\n    diff = centers[:, :, np.newaxis, :] - centers[:, np.newaxis, :, :]\n    dist = np.linalg.norm(diff, axis=-1)\n    \n    for _ in range(5000):\n        # Trim boundaries strictly\n        radii = np.minimum(radii, centers[..., 0])\n        radii = np.minimum(radii, 1.0 - centers[..., 0])\n        radii = np.minimum(radii, centers[..., 1])\n        radii = np.minimum(radii, 1.0 - centers[..., 1])\n\n        # Trim pairwise overlaps\n        sum_r = radii[:, :, np.newaxis] + radii[:, np.newaxis, :]\n        overlap = np.maximum(0, sum_r - dist)\n        overlap = overlap * (1.0 - mask)\n        \n        max_over = np.max(overlap, axis=2)\n        if np.max(max_over) < 1e-11:\n            break\n            \n        # Shrink by slightly more than half of the worst overlap to guarantee convergence\n        radii -= max_over * 0.505\n        radii = np.maximum(radii, 0.0)\n\n    # Return the best batch result\n    sum_radii = np.sum(radii, axis=1)\n    best_idx = int(np.argmax(sum_radii))\n\n    return centers[best_idx], radii[best_idx], float(sum_radii[best_idx])\n```\nUnique approach: Modification:, [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.3333, Type: Exploratory)\n```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\n\ndef run_packing():\n    \"\"\"\n    Construct an optimized arrangement of 26 circles in a unit square\n    to maximize the sum of their radii.\n\n    Returns:\n        Tuple of (centers, radii, sum_of_radii)\n        centers: np.array of shape (26, 2) with (x, y) coordinates\n        radii: np.array of shape (26) with radius of each circle\n        sum_of_radii: Sum of all radii\n    \"\"\"\n    np.random.seed(42)\n    B = 100\n    N = 26\n\n    # Initialization\n    centers = np.random.uniform(0.1, 0.9, size=(B, N, 2))\n\n    # Pattern 1: Edge/Corner biased (Beta distribution)\n    centers[0:20] = np.random.beta(0.3, 0.3, size=(20, N, 2))\n\n    # Pattern 2: Uniform grid roughly tailored for square packing\n    xv, yv = np.meshgrid(np.linspace(0.1, 0.9, 6), np.linspace(0.1, 0.9, 5))\n    grid_pts = np.stack([xv.flatten(), yv.flatten()], axis=-1)\n    for b in range(20, 40):\n        idx = np.random.choice(len(grid_pts), N, replace=False)\n        centers[b] = grid_pts[idx] + np.random.normal(0, 0.01, size=(N, 2))\n\n    # Pattern 3: Concentric rings placement\n    for b in range(40, 60):\n        centers[b, 0] = [0.5, 0.5]\n        idx = 1\n        angle_offset_1 = np.random.uniform(0, 2 * np.pi)\n        for i in range(7):\n            theta = 2.0 * np.pi * i / 7.0 + angle_offset_1\n            centers[b, idx] = [0.5 + 0.22 * np.cos(theta),\n                               0.5 + 0.22 * np.sin(theta)]\n            idx += 1\n        angle_offset_2 = np.random.uniform(0, 2 * np.pi)\n        for i in range(18):\n            theta = 2.0 * np.pi * i / 18.0 + angle_offset_2\n            centers[b, idx] = [0.5 + 0.44 * np.cos(theta),\n                               0.5 + 0.44 * np.sin(theta)]\n            idx += 1\n        centers[b] += np.random.normal(0, 0.005, size=(N, 2))\n\n    # Pattern 4: Golden ratio spiral (Fibonacci)\n    phi = (1.0 + np.sqrt(5.0)) / 2.0\n    for b in range(60, 80):\n        angle_offset = np.random.uniform(0, 2 * np.pi)\n        for i in range(N):\n            r_dist = np.sqrt((i + 0.5) / N) * 0.45\n            theta = 2.0 * np.pi * i / phi + angle_offset\n            centers[b, i, 0] = 0.5 + r_dist * np.cos(theta)\n            centers[b, i, 1] = 0.5 + r_dist * np.sin(theta)\n        centers[b] += np.random.normal(0, 0.01, size=(N, 2))\n\n    # Pattern 5: Random uniform with noise\n    centers[80:100] = np.random.uniform(0.05, 0.95, size=(20, N, 2))\n\n    # Keep all centers in a safe bounding box initially\n    centers = np.clip(centers, 0.02, 0.98)\n\n    # Size placement bias: larger radii in the center, smaller in corners/edges\n    dist_to_center = np.linalg.norm(centers - 0.5, axis=-1)\n    radii = 0.08 - 0.05 * (dist_to_center / 0.707)\n    radii = np.clip(radii, 0.02, 0.1)\n\n    # Adam Optimizer states\n    lr_initial = 0.02\n    beta1 = 0.9\n    beta2 = 0.999\n    eps = 1e-8\n\n    m_c = np.zeros_like(centers)\n    v_c = np.zeros_like(centers)\n    m_r = np.zeros_like(radii)\n    v_r = np.zeros_like(radii)\n\n    max_steps = 20000\n    lam_start = 5.0\n    lam_end = 2e6\n    lam_factor = (lam_end / lam_start) ** (1.0 / max_steps)\n\n    mask = np.eye(N)[np.newaxis, :, :]\n\n    for step in range(max_steps):\n        # Simulated annealing for constraints: penalty weight grows exponentially\n        lam = lam_start * (lam_factor ** step)\n        \n        # Smoothly decaying learning rate\n        lr = lr_initial * (0.01 ** (step / max_steps))\n\n        # Break perfect symmetry to escape local maxima\n        if step < max_steps // 2 and step % 500 == 0:\n            centers += np.random.normal(0, 0.001, size=centers.shape)\n            centers = np.clip(centers, 0.0, 1.0)\n\n        # Pairwise differences and distances\n        diff = centers[:, :, np.newaxis, :] - centers[:, np.newaxis, :, :]\n        dist = np.linalg.norm(diff, axis=-1)\n\n        # Calculate overlap magnitudes\n        sum_r = radii[:, :, np.newaxis] + radii[:, np.newaxis, :]\n        overlap = np.maximum(0, sum_r - dist)\n        overlap = overlap * (1.0 - mask)  # Exclude self-intersections\n\n        # Gradients w.r.t pairwise overlaps\n        grad_r_overlap = 2.0 * np.sum(overlap, axis=2)\n\n        dist_safe = dist + mask + 1e-8\n        force_mag = 2.0 * overlap / dist_safe\n        grad_c_overlap = -np.sum(force_mag[..., np.newaxis] * diff, axis=2)\n\n        # Boundary constraints\n        x = centers[..., 0]\n        y = centers[..., 1]\n        r = radii\n\n        p_L = np.maximum(0, r - x)\n        p_R = np.maximum(0, x + r - 1.0)\n        p_B = np.maximum(0, r - y)\n        p_T = np.maximum(0, y + r - 1.0)\n\n        grad_r_bounds = 2.0 * (p_L + p_R + p_B + p_T)\n        grad_x_bounds = -2.0 * p_L + 2.0 * p_R\n        grad_y_bounds = -2.0 * p_B + 2.0 * p_T\n        grad_c_bounds = np.stack([grad_x_bounds, grad_y_bounds], axis=-1)\n\n        # Combine gradients. Objective is maximizing sum(radii) -> min -sum(r)\n        grad_r = -1.0 + lam * (grad_r_overlap + grad_r_bounds)\n        grad_c = lam * (grad_c_overlap + grad_c_bounds)\n\n        # Clip gradients to prevent numeric explosion\n        grad_r = np.clip(grad_r, -1e4, 1e4)\n        grad_c = np.clip(grad_c, -1e4, 1e4)\n\n        # Apply Adam updates\n        m_c = beta1 * m_c + (1 - beta1) * grad_c\n        v_c = beta2 * v_c + (1 - beta2) * (grad_c ** 2)\n        centers -= lr * m_c / (np.sqrt(v_c) + eps)\n\n        m_r = beta1 * m_r + (1 - beta1) * grad_r\n        v_r = beta2 * v_r + (1 - beta2) * (grad_r ** 2)\n        radii -= lr * m_r / (np.sqrt(v_r) + eps)\n\n        # Enforce valid physical ranges to maintain optimizer stability\n        radii = np.maximum(radii, 0.001)\n        centers = np.clip(centers, 0.0, 1.0)\n\n    # Final pass: Linear Programming for exact mathematical validity and maximization\n    best_sum = -1.0\n    best_centers = None\n    best_radii = None\n\n    for b in range(B):\n        c = np.clip(centers[b], 0.0, 1.0)\n        c_obj = -np.ones(N)\n        A_ub = []\n        b_ub = []\n\n        # Bound constraints per circle\n        for i in range(N):\n            x_i, y_i = c[i]\n            \n            # Left bound\n            row = np.zeros(N)\n            row[i] = 1.0\n            A_ub.append(row)\n            b_ub.append(x_i)\n            \n            # Right bound\n            row = np.zeros(N)\n            row[i] = 1.0\n            A_ub.append(row)\n            b_ub.append(1.0 - x_i)\n            \n            # Bottom bound\n            row = np.zeros(N)\n            row[i] = 1.0\n            A_ub.append(row)\n            b_ub.append(y_i)\n            \n            # Top bound\n            row = np.zeros(N)\n            row[i] = 1.0\n            A_ub.append(row)\n            b_ub.append(1.0 - y_i)\n\n        # Pairwise distance constraints to ensure no circle overlaps\n        for i in range(N):\n            for j in range(i + 1, N):\n                dist_ij = np.linalg.norm(c[i] - c[j])\n                row = np.zeros(N)\n                row[i] = 1.0\n                row[j] = 1.0\n                A_ub.append(row)\n                b_ub.append(dist_ij)\n\n        A_ub = np.array(A_ub)\n        b_ub = np.array(b_ub)\n\n        res = linprog(c_obj, A_ub=A_ub, b_ub=b_ub,\n                      bounds=(0.0, None), method='highs')\n\n        if res.success:\n            s_r = -res.fun\n            if s_r > best_sum:\n                best_sum = s_r\n                best_centers = c\n                best_radii = res.x\n\n    # Fallback to mathematically rigorous manual trimming if LP somehow fails\n    if best_centers is None:\n        best_centers = centers[0]\n        best_radii = radii[0]\n        for _ in range(1500):\n            best_radii = np.minimum(best_radii, best_centers[..., 0])\n            best_radii = np.minimum(best_radii, 1.0 - best_centers[..., 0])\n            best_radii = np.minimum(best_radii, best_centers[..., 1])\n            best_radii = np.minimum(best_radii, 1.0 - best_centers[..., 1])\n\n            for i in range(N):\n                for j in range(i + 1, N):\n                    dist_ij = np.linalg.norm(best_centers[i] - best_centers[j])\n                    if best_radii[i] + best_radii[j] > dist_ij:\n                        overlap_ij = best_radii[i] + best_radii[j] - dist_ij\n                        # Reduce each radius symmetrically\n                        best_radii[i] -= overlap_ij * 0.505\n                        best_radii[j] -= overlap_ij * 0.505\n            best_radii = np.maximum(best_radii, 0.0)\n        best_sum = np.sum(best_radii)\n\n    return best_centers, best_radii, float(best_sum)\n\n\n# Create an alias to ensure maximum compatibility with the evaluator hook\nconstruct_packing = run_packing\n```\nUnique approach: Modification:, [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\n\ndef construct_packing():\n    \"\"\"\n    Construct an optimized arrangement of 26 circles in a unit square\n    to maximize the sum of their radii.\n\n    Returns:\n        Tuple of (centers, radii, sum_of_radii)\n        centers: np.array of shape (26, 2) with (x, y) coordinates\n        radii: np.array of shape (26) with radius of each circle\n        sum_of_radii: Sum of all radii\n    \"\"\"\n    np.random.seed(42)\n    B = 96  # Batch size for parallel exploration\n    N = 26  # Number of circles\n\n    centers = np.random.uniform(0.1, 0.9, size=(B, N, 2))\n\n    # Pattern 1: Edge/Corner biased (Beta distribution)\n    centers[0:16] = np.random.beta(0.3, 0.3, size=(16, N, 2))\n\n    # Pattern 2: Concentric rings placement\n    for b in range(16, 32):\n        centers[b, 0] = [0.5, 0.5]\n        idx = 1\n        angle_offset_1 = np.random.uniform(0, 2 * np.pi)\n        for i in range(7):\n            theta = 2.0 * np.pi * i / 7.0 + angle_offset_1\n            centers[b, idx] = [0.5 + 0.22 * np.cos(theta),\n                               0.5 + 0.22 * np.sin(theta)]\n            idx += 1\n        angle_offset_2 = np.random.uniform(0, 2 * np.pi)\n        for i in range(18):\n            theta = 2.0 * np.pi * i / 18.0 + angle_offset_2\n            centers[b, idx] = [0.5 + 0.44 * np.cos(theta),\n                               0.5 + 0.44 * np.sin(theta)]\n            idx += 1\n        centers[b] += np.random.normal(0, 0.005, size=(N, 2))\n\n    # Pattern 3: Golden ratio spiral (Fibonacci)\n    phi = (1.0 + np.sqrt(5.0)) / 2.0\n    for b in range(32, 48):\n        angle_offset = np.random.uniform(0, 2 * np.pi)\n        for i in range(N):\n            r_dist = np.sqrt((i + 0.5) / N) * 0.45\n            theta = 2.0 * np.pi * i / phi + angle_offset\n            centers[b, i, 0] = 0.5 + r_dist * np.cos(theta)\n            centers[b, i, 1] = 0.5 + r_dist * np.sin(theta)\n        centers[b] += np.random.normal(0, 0.01, size=(N, 2))\n\n    # Pattern 4: Hexagonal grid roughly tailored for square packing\n    hex_centers = []\n    xv, yv = np.meshgrid(np.linspace(0.05, 0.95, 7), np.linspace(0.05, 0.95, 7))\n    xv[1::2] += (0.9 / 6) / 2.0  # Shift alternate rows\n    hex_pts = np.stack([xv.flatten(), yv.flatten()], axis=-1)\n    hex_pts = hex_pts[(hex_pts[:, 0] <= 0.95) & (hex_pts[:, 1] <= 0.95)]\n    for b in range(48, 64):\n        idx = np.random.choice(len(hex_pts), N, replace=False)\n        centers[b] = hex_pts[idx] + np.random.normal(0, 0.005, size=(N, 2))\n\n    # Pattern 5: Standard uniform grid\n    xv, yv = np.meshgrid(np.linspace(0.08, 0.92, 6), np.linspace(0.08, 0.92, 6))\n    grid_pts = np.stack([xv.flatten(), yv.flatten()], axis=-1)\n    for b in range(64, 80):\n        idx = np.random.choice(len(grid_pts), N, replace=False)\n        centers[b] = grid_pts[idx] + np.random.normal(0, 0.01, size=(N, 2))\n\n    # Pattern 6: Random uniform with noise\n    centers[80:96] = np.random.uniform(0.05, 0.95, size=(16, N, 2))\n\n    # Keep all centers in a safe bounding box initially\n    np.clip(centers, 0.02, 0.98, out=centers)\n\n    # Size placement bias: larger radii in the center, smaller in corners/edges\n    dist_to_center = np.linalg.norm(centers - 0.5, axis=-1)\n    radii = 0.08 - 0.05 * (dist_to_center / 0.707)\n    np.clip(radii, 0.02, 0.1, out=radii)\n\n    # Adam Optimizer states\n    lr_initial = 0.02\n    beta1 = 0.9\n    beta2 = 0.999\n    eps = 1e-8\n\n    m_c = np.zeros_like(centers)\n    v_c = np.zeros_like(centers)\n    m_r = np.zeros_like(radii)\n    v_r = np.zeros_like(radii)\n\n    max_steps = 12000\n    lam_start = 5.0\n    lam_end = 2e6\n    lam_factor = (lam_end / lam_start) ** (1.0 / max_steps)\n\n    diag_indices = np.arange(N)\n\n    for step in range(max_steps):\n        # Simulated annealing for constraints: penalty weight grows exponentially\n        lam = lam_start * (lam_factor ** step)\n        \n        # Smoothly decaying learning rate, ending very fine to settle micro-adjustments\n        lr = lr_initial * (0.0005 ** (step / max_steps))\n\n        # Break perfect symmetry to escape local maxima\n        if 0 < step < max_steps // 2 and step % 500 == 0:\n            centers += np.random.normal(0, 0.001, size=centers.shape)\n            np.clip(centers, 0.0, 1.0, out=centers)\n\n        # Pairwise differences and fast distance calculation\n        diff = centers[:, :, np.newaxis, :] - centers[:, np.newaxis, :, :]\n        dist = np.sqrt(np.sum(diff * diff, axis=-1))\n        \n        # Ignore self-intersection by setting diagonal distance arbitrarily high\n        dist[:, diag_indices, diag_indices] = 10.0\n\n        # Calculate overlap magnitudes\n        sum_r = radii[:, :, np.newaxis] + radii[:, np.newaxis, :]\n        overlap = np.maximum(0, sum_r - dist)\n\n        # Gradients w.r.t pairwise overlaps\n        grad_r_overlap = 2.0 * np.sum(overlap, axis=2)\n\n        force_mag = 2.0 * overlap / (dist + 1e-8)\n        grad_c_overlap = -np.sum(force_mag[..., np.newaxis] * diff, axis=2)\n\n        # Boundary constraints\n        x = centers[..., 0]\n        y = centers[..., 1]\n        r = radii\n\n        p_L = np.maximum(0, r - x)\n        p_R = np.maximum(0, x + r - 1.0)\n        p_B = np.maximum(0, r - y)\n        p_T = np.maximum(0, y + r - 1.0)\n\n        grad_r_bounds = 2.0 * (p_L + p_R + p_B + p_T)\n        grad_x_bounds = -2.0 * p_L + 2.0 * p_R\n        grad_y_bounds = -2.0 * p_B + 2.0 * p_T\n        grad_c_bounds = np.stack([grad_x_bounds, grad_y_bounds], axis=-1)\n\n        # Combine gradients. Objective is maximizing sum(radii) -> min -sum(r)\n        grad_r = -1.0 + lam * (grad_r_overlap + grad_r_bounds)\n        grad_c = lam * (grad_c_overlap + grad_c_bounds)\n\n        # Clip gradients to prevent numeric explosion\n        np.clip(grad_r, -1e4, 1e4, out=grad_r)\n        np.clip(grad_c, -1e4, 1e4, out=grad_c)\n\n        # Apply in-place Adam updates for computation speed\n        m_c *= beta1\n        m_c += (1.0 - beta1) * grad_c\n        v_c *= beta2\n        v_c += (1.0 - beta2) * (grad_c * grad_c)\n        centers -= lr * m_c / (np.sqrt(v_c) + eps)\n\n        m_r *= beta1\n        m_r += (1.0 - beta1) * grad_r\n        v_r *= beta2\n        v_r += (1.0 - beta2) * (grad_r * grad_r)\n        radii -= lr * m_r / (np.sqrt(v_r) + eps)\n\n        # Enforce valid physical ranges to maintain optimizer stability\n        np.maximum(radii, 0.001, out=radii)\n        np.clip(centers, 0.0, 1.0, out=centers)\n\n    # Final pass: Linear Programming for exact mathematical validity and maximization\n    best_sum = -1.0\n    best_centers = None\n    best_radii = None\n\n    idx_i, idx_j = np.triu_indices(N, 1)\n    A_ub_base = np.zeros((len(idx_i), N))\n    A_ub_base[np.arange(len(idx_i)), idx_i] = 1.0\n    A_ub_base[np.arange(len(idx_i)), idx_j] = 1.0\n\n    # Sort batches and only apply linprog to the top candidates to save compute\n    batch_scores = np.sum(radii, axis=1)\n    top_batches = np.argsort(batch_scores)[-20:][::-1]\n\n    for b in top_batches:\n        c = np.clip(centers[b], 0.0, 1.0)\n        c_obj = -np.ones(N)\n        \n        bounds = []\n        # Bound constraints per circle mapped natively to LP bounds for speed\n        for i in range(N):\n            x_i, y_i = c[i]\n            max_r = max(0.0, float(min(x_i, 1.0 - x_i, y_i, 1.0 - y_i)))\n            bounds.append((0.0, max_r))\n\n        b_ub = np.linalg.norm(c[idx_i] - c[idx_j], axis=1)\n\n        try:\n            res = linprog(c_obj, A_ub=A_ub_base, b_ub=b_ub,\n                          bounds=bounds, method='highs')\n            if res.success:\n                s_r = -res.fun\n                if s_r > best_sum:\n                    best_sum = s_r\n                    best_centers = c.copy()\n                    best_radii = res.x.copy()\n        except Exception:\n            pass\n\n    # Fallback to mathematically rigorous manual trimming if LP somehow fails entirely\n    if best_centers is None:\n        best_idx = int(top_batches[0])\n        best_centers = np.clip(centers[best_idx], 0.0, 1.0)\n        best_radii = np.maximum(radii[best_idx], 0.0)\n        for _ in range(1500):\n            best_radii = np.minimum(best_radii, best_centers[..., 0])\n            best_radii = np.minimum(best_radii, 1.0 - best_centers[..., 0])\n            best_radii = np.minimum(best_radii, best_centers[..., 1])\n            best_radii = np.minimum(best_radii, 1.0 - best_centers[..., 1])\n\n            for i in range(N):\n                for j in range(i + 1, N):\n                    dist_ij = np.linalg.norm(best_centers[i] - best_centers[j])\n                    if best_radii[i] + best_radii[j] > dist_ij:\n                        overlap_ij = best_radii[i] + best_radii[j] - dist_ij\n                        # Reduce each radius symmetrically\n                        best_radii[i] -= overlap_ij * 0.505\n                        best_radii[j] -= overlap_ij * 0.505\n            best_radii = np.maximum(best_radii, 0.0)\n        best_sum = np.sum(best_radii)\n\n    return best_centers, best_radii, float(best_sum)\n\n\n# Create an alias to ensure maximum compatibility with the evaluator hook\nrun_packing = construct_packing\n```\n\n# Task\nRewrite the program to improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```", "responses": ["An elegant and highly effective approach to boosting the fitness score for this packing problem is to introduce a secondary, precise non-linear optimization phase. \n\nWhile the Adam optimizer is excellent at aggressively moving the 26 circles to find a macro-scale non-overlapping layout, it struggles with extremely fine numerical precision constraints at convergence (vanishing gradients). To bridge this gap, we can take the most promising candidate configurations outputted by Adam and feed them into Scipy's Sequential Least Squares Programming (SLSQP). We supply SLSQP with exact, mathematically precise Jacobian derivatives. This allows the solver to slide perfectly along the tight constraint bounds (packing the circles flush against each other and the walls) and fully extract the maximum mathematically possible radius configuration. \n\nLastly, we ensure that absolute constraint validity is maintained by feeding the SLSQP output through a final Linear Programming trim.\n\n### Improved Program Code\n\n```python\nimport numpy as np\nfrom scipy.optimize import linprog, minimize\n\ndef get_slsqp_funcs(N):\n    \"\"\"\n    Constructs and returns the exact objective and constraint functions\n    (along with their Jacobians) needed for precise SLSQP optimization.\n    \"\"\"\n    idx_i, idx_j = np.triu_indices(N, 1)\n    \n    def obj(v):\n        return -np.sum(v[2*N:])\n        \n    def jac_obj(v):\n        j = np.zeros_like(v)\n        j[2*N:] = -1.0\n        return j\n\n    def constr(v):\n        x = v[:N]\n        y = v[N:2*N]\n        r = v[2*N:]\n        \n        # Boundary constraints\n        c_bounds = np.concatenate([\n            x - r,\n            1.0 - x - r,\n            y - r,\n            1.0 - y - r\n        ])\n        \n        # Overlap constraints: dx^2 + dy^2 >= (r_i + r_j)^2\n        dx = x[idx_i] - x[idx_j]\n        dy = y[idx_i] - y[idx_j]\n        sr = r[idx_i] + r[idx_j]\n        c_overlap = dx**2 + dy**2 - sr**2\n        \n        return np.concatenate([c_bounds, c_overlap])\n        \n    def constr_jac(v):\n        x = v[:N]\n        y = v[N:2*N]\n        r = v[2*N:]\n        \n        J = np.zeros((4*N + len(idx_i), 3*N))\n        rng = np.arange(N)\n        \n        # Gradients for boundary constraints\n        J[rng, rng] = 1.0\n        J[rng, 2*N + rng] = -1.0\n        \n        J[N + rng, rng] = -1.0\n        J[N + rng, 2*N + rng] = -1.0\n        \n        J[2*N + rng, N + rng] = 1.0\n        J[2*N + rng, 2*N + rng] = -1.0\n        \n        J[3*N + rng, N + rng] = -1.0\n        J[3*N + rng, 2*N + rng] = -1.0\n        \n        # Gradients for pairwise distance constraints\n        dx = x[idx_i] - x[idx_j]\n        "]}}, "artifacts_json": "{\"stderr\": \"Stage 1 evaluation failed: Program execution failed: unterminated string literal (detected at line 3) (tmp5cr1g_fv.py, line 3)\", \"traceback\": \"Traceback (most recent call last):\\n  File \\\"/home/jupyter/Code/oe/dennis/run2_improved_si/evaluator.py\\\", line 384, in evaluate_stage1\\n    centers, radii, sum_radii = run_with_timeout(program_path, timeout_seconds=600)\\n  File \\\"/home/jupyter/Code/oe/dennis/run2_improved_si/evaluator.py\\\", line 172, in run_with_timeout\\n    raise RuntimeError(f\\\"Program execution failed: {results['error']}\\\")\\nRuntimeError: Program execution failed: unterminated string literal (detected at line 3) (tmp5cr1g_fv.py, line 3)\\n\", \"failure_stage\": \"stage1_execution\", \"suggestion\": \"Check basic syntax and imports before attempting full evaluation\"}", "artifact_dir": null, "embedding": null}