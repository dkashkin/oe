{"id": "997ea153-70e3-49d8-acd6-e83f726e464c", "code": "# EVOLVE-BLOCK-START\n\"\"\"Constructor-based iterative circle packing optimization for n=26 circles\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\n\ndef get_loss_and_grads(X, Y, R, c_t):\n    \"\"\"\n    Computes penalty-based loss and vectorized gradients to evaluate boundaries and overlaps.\n    Penalty weight `c_t` strictly forces constraints at later stages to create proper layouts.\n    \"\"\"\n    grad_X = np.zeros_like(X)\n    grad_Y = np.zeros_like(Y)\n    \n    # Maximize sum(R) -> Equivalent to minimizing -sum(R)\n    grad_R = np.full_like(R, -1.0)\n    loss = -np.sum(R)\n    \n    # Square Boundary Restrictions [0, 1]\n    viol_xmin = R - X\n    mask_xmin = viol_xmin > 0\n    if np.any(mask_xmin):\n        grad_R[mask_xmin] += c_t * 2.0 * viol_xmin[mask_xmin]\n        grad_X[mask_xmin] -= c_t * 2.0 * viol_xmin[mask_xmin]\n        loss += c_t * np.sum(viol_xmin[mask_xmin]**2)\n        \n    viol_xmax = X + R - 1.0\n    mask_xmax = viol_xmax > 0\n    if np.any(mask_xmax):\n        grad_R[mask_xmax] += c_t * 2.0 * viol_xmax[mask_xmax]\n        grad_X[mask_xmax] += c_t * 2.0 * viol_xmax[mask_xmax]\n        loss += c_t * np.sum(viol_xmax[mask_xmax]**2)\n        \n    viol_ymin = R - Y\n    mask_ymin = viol_ymin > 0\n    if np.any(mask_ymin):\n        grad_R[mask_ymin] += c_t * 2.0 * viol_ymin[mask_ymin]\n        grad_Y[mask_ymin] -= c_t * 2.0 * viol_ymin[mask_ymin]\n        loss += c_t * np.sum(viol_ymin[mask_ymin]**2)\n        \n    viol_ymax = Y + R - 1.0\n    mask_ymax = viol_ymax > 0\n    if np.any(mask_ymax):\n        grad_R[mask_ymax] += c_t * 2.0 * viol_ymax[mask_ymax]\n        grad_Y[mask_ymax] += c_t * 2.0 * viol_ymax[mask_ymax]\n        loss += c_t * np.sum(viol_ymax[mask_ymax]**2)\n        \n    # Non-overlapping circles Constraints\n    dx = X[:, None] - X[None, :]\n    dy = Y[:, None] - Y[None, :]\n    dist2 = dx**2 + dy**2\n    np.fill_diagonal(dist2, 1.0)\n    \n    # Float stability safeguards\n    dist = np.maximum(np.sqrt(dist2), 1e-10)\n    \n    sum_R = R[:, None] + R[None, :]\n    viol_overlap = sum_R - dist\n    np.fill_diagonal(viol_overlap, -1.0) # ignore self-overlaps\n    \n    mask_overlap = viol_overlap > 0\n    \n    if np.any(mask_overlap):\n        loss += c_t * np.sum(viol_overlap[mask_overlap]**2) / 2.0\n        \n        # Derivatives for pairs\n        factor = c_t * 2.0 * (viol_overlap * mask_overlap)\n        grad_R += np.sum(factor, axis=1)\n        \n        factor_dist = factor / dist\n        grad_X += np.sum(-factor_dist * dx, axis=1)\n        grad_Y += np.sum(-factor_dist * dy, axis=1)\n        \n    return loss, grad_X, grad_Y, grad_R\n\n\ndef make_valid(X, Y, R):\n    \"\"\"\n    Safely forces parameters down onto an exactly strict feasible regime mathematically,\n    shrinking only physically colliding or bound-infringing objects efficiently.\n    \"\"\"\n    N = len(R)\n    # Global explicit clamp bounds:\n    R = np.minimum.reduce([R, X, 1.0 - X, Y, 1.0 - Y])\n    R = np.maximum(R, 1e-6)\n    \n    for _ in range(50):\n        dx = X[:, None] - X[None, :]\n        dy = Y[:, None] - Y[None, :]\n        dist = np.sqrt(dx**2 + dy**2)\n        np.fill_diagonal(dist, 10.0)\n        \n        sum_R = R[:, None] + R[None, :]\n        overlap = sum_R - dist\n        \n        if np.max(overlap) <= 1e-8:\n            break\n            \n        indices = np.argsort(overlap.flatten())[::-1]\n        for idx in indices:\n            i, j = divmod(int(idx), N)\n            if i >= j:\n                continue\n            \n            curr_overlap = R[i] + R[j] - dist[i, j]\n            if curr_overlap > 1e-9:\n                scale = dist[i, j] / (R[i] + R[j])\n                R[i] *= scale\n                R[j] *= scale\n                \n    # Exhaustive final pair-clashing scan ensuring absolute float-truncation constraint truth\n    for i in range(N):\n        for j in range(i + 1, N):\n            d = np.sqrt((X[i]-X[j])**2 + (Y[i]-Y[j])**2)\n            if R[i] + R[j] > d:\n                scale = d / (R[i] + R[j])\n                R[i] *= scale\n                R[j] *= scale\n                \n    R = np.minimum.reduce([R, X, 1.0 - X, Y, 1.0 - Y])\n    R = np.maximum(R, 0.0)\n    R *= 0.999999 # Micro padding entirely offsets precision drift\n    \n    return X, Y, R\n\n\ndef polish_slsqp(X, Y, R):\n    \"\"\"\n    Fine-tunes mathematically feasible solutions continuously into strict optima.\n    Utilizes localized L-BFGS constrained projections heavily pushing radius magnitudes upwards\n    towards maximum configuration geometries smoothly against identical local configurations.\n    \"\"\"\n    N = len(R)\n    \n    def obj(p):\n        return -np.sum(p[2*N:])\n        \n    def grad_obj(p):\n        g = np.zeros(3*N)\n        g[2*N:] = -1.0\n        return g\n        \n    bounds = [(0.0, 1.0)] * (2*N) + [(0.001, 1.0)] * N\n    \n    def boundary_cons(p):\n        X_p = p[:N]\n        Y_p = p[N:2*N]\n        R_p = p[2*N:]\n        return np.concatenate([\n            X_p - R_p,\n            1.0 - X_p - R_p,\n            Y_p - R_p,\n            1.0 - Y_p - R_p\n        ])\n        \n    def boundary_jac(p):\n        J = np.zeros((4*N, 3*N))\n        idx = np.arange(N)\n        # Vectorized assignment representing explicit border constraint components respectively\n        J[idx, idx] = 1.0\n        J[idx, 2*N + idx] = -1.0\n        J[N + idx, idx] = -1.0\n        J[N + idx, 2*N + idx] = -1.0\n        J[2*N + idx, N + idx] = 1.0\n        J[2*N + idx, 2*N + idx] = -1.0\n        J[3*N + idx, N + idx] = -1.0\n        J[3*N + idx, 2*N + idx] = -1.0\n        return J\n\n    def overlap_cons(p):\n        X_p = p[:N]\n        Y_p = p[N:2*N]\n        R_p = p[2*N:]\n        i_idx, j_idx = np.triu_indices(N, k=1)\n        dist2 = (X_p[i_idx] - X_p[j_idx])**2 + (Y_p[i_idx] - Y_p[j_idx])**2\n        return dist2 - (R_p[i_idx] + R_p[j_idx])**2\n        \n    def overlap_jac(p):\n        X_p = p[:N]\n        Y_p = p[N:2*N]\n        R_p = p[2*N:]\n        i_idx, j_idx = np.triu_indices(N, k=1)\n        num_pairs = len(i_idx)\n        J = np.zeros((num_pairs, 3*N))\n        \n        dx = X_p[i_idx] - X_p[j_idx]\n        dy = Y_p[i_idx] - Y_p[j_idx]\n        dr = R_p[i_idx] + R_p[j_idx]\n        \n        pair_range = np.arange(num_pairs)\n        J[pair_range, i_idx] = 2.0 * dx\n        J[pair_range, j_idx] = -2.0 * dx\n        J[pair_range, N + i_idx] = 2.0 * dy\n        J[pair_range, N + j_idx] = -2.0 * dy\n        J[pair_range, 2*N + i_idx] = -2.0 * dr\n        J[pair_range, 2*N + j_idx] = -2.0 * dr\n        return J\n\n    cons = [\n        {'type': 'ineq', 'fun': boundary_cons, 'jac': boundary_jac},\n        {'type': 'ineq', 'fun': overlap_cons, 'jac': overlap_jac}\n    ]\n    \n    p0 = np.concatenate([X, Y, R])\n    res = minimize(\n        obj, p0, method='SLSQP', jac=grad_obj, bounds=bounds,\n        constraints=cons, options={'maxiter': 400, 'ftol': 1e-6}\n    )\n    \n    p = res.x\n    return p[:N], p[N:2*N], p[2*N:]\n\n\ndef construct_packing():\n    \"\"\"\n    Implements simulated annealing optimization combined with distinct symmetry-breaking seeds.\n    Final solutions iteratively passed through deterministic physical expansion maximizations utilizing SQP bounds mapping natively.\n    \"\"\"\n    best_sum = -1.0\n    best_X = None\n    best_Y = None\n    best_R = None\n    \n    num_runs = 16\n    N = 26\n    candidates = []\n    \n    for run in range(num_runs):\n        np.random.seed(42 + run)\n        \n        # Sequentially map distinctly varied geometric layout arrangements forcing extensive diverse searches initially\n        X_init, Y_init = [], []\n        if run % 4 == 0:\n            X_init.extend([0.05, 0.95, 0.05, 0.95])\n            Y_init.extend([0.05, 0.05, 0.95, 0.95])\n            for _ in range(N - 4):\n                X_init.append(np.random.uniform(0.1, 0.9))\n                Y_init.append(np.random.uniform(0.1, 0.9))\n        elif run % 4 == 1:\n            for _ in range(N):\n                X_init.append(np.random.uniform(0.05, 0.95))\n                Y_init.append(np.random.uniform(0.05, 0.95))\n        elif run % 4 == 2:\n            n_outer = 12\n            for i in range(n_outer):\n                th = 2 * np.pi * i / n_outer\n                X_init.append(0.5 + 0.38 * np.cos(th))\n                Y_init.append(0.5 + 0.38 * np.sin(th))\n            for _ in range(N - n_outer):\n                X_init.append(np.random.uniform(0.25, 0.75))\n                Y_init.append(np.random.uniform(0.25, 0.75))\n        else:\n            grid_dim = int(np.ceil(np.sqrt(N)))\n            for i in range(N):\n                r_pos = i // grid_dim\n                c_pos = i % grid_dim\n                X_init.append(0.1 + 0.8 * r_pos / max(1, grid_dim - 1))\n                Y_init.append(0.1 + 0.8 * c_pos / max(1, grid_dim - 1))\n            \n        X = np.array(X_init[:N])\n        Y = np.array(Y_init[:N])\n        \n        if run % 2 == 0:\n            dist_c = np.sqrt((X - 0.5)**2 + (Y - 0.5)**2)\n            R = 0.06 - 0.04 * dist_c\n        else:\n            R = np.random.uniform(0.02, 0.06, N)\n        \n        # Adaptive gradient descent hyperparameters structurally locking positions down gracefully.\n        num_steps = 2000\n        m_X, v_X = np.zeros_like(X), np.zeros_like(X)\n        m_Y, v_Y = np.zeros_like(Y), np.zeros_like(Y)\n        m_R, v_R = np.zeros_like(R), np.zeros_like(R)\n        \n        lr_init = 0.015\n        beta1, beta2 = 0.9, 0.999\n        epsilon = 1e-8\n        \n        c_t = 10.0\n        c_max = 500000.0 \n        c_mult = (c_max / c_t) ** (1.0 / num_steps)\n        \n        for t in range(1, num_steps + 1):\n            curr_lr = lr_init * (0.01 ** (t / num_steps))\n            \n            _, g_X, g_Y, g_R = get_loss_and_grads(X, Y, R, c_t)\n            \n            m_X = beta1 * m_X + (1 - beta1) * g_X\n            v_X = beta2 * v_X + (1 - beta2) * (g_X ** 2)\n            m_X_h = m_X / (1 - beta1 ** t)\n            v_X_h = v_X / (1 - beta2 ** t)\n            X -= curr_lr * m_X_h / (np.sqrt(v_X_h) + epsilon)\n            \n            m_Y = beta1 * m_Y + (1 - beta1) * g_Y\n            v_Y = beta2 * v_Y + (1 - beta2) * (g_Y ** 2)\n            m_Y_h = m_Y / (1 - beta1 ** t)\n            v_Y_h = v_Y / (1 - beta2 ** t)\n            Y -= curr_lr * m_Y_h / (np.sqrt(v_Y_h) + epsilon)\n            \n            m_R = beta1 * m_R + (1 - beta1) * g_R\n            v_R = beta2 * v_R + (1 - beta2) * (g_R ** 2)\n            m_R_h = m_R / (1 - beta1 ** t)\n            v_R_h = v_R / (1 - beta2 ** t)\n            R -= curr_lr * m_R_h / (np.sqrt(v_R_h) + epsilon)\n            \n            X = np.clip(X, 0.0, 1.0)\n            Y = np.clip(Y, 0.0, 1.0)\n            R = np.maximum(R, 0.001)\n            \n            c_t *= c_mult\n            \n            # Sub-stage random topological perturbation jitter dynamically preventing edge-constraint trapping effectively natively\n            if t % 300 == 0 and t < 1200:\n                X += np.random.normal(0, 0.002, N)\n                Y += np.random.normal(0, 0.002, N)\n                X = np.clip(X, 0.0, 1.0)\n                Y = np.clip(Y, 0.0, 1.0)\n                \n        # Rigid bounds cleanly formatting true feasibility states conditionally ready to deploy strictly via refinement\n        X_val, Y_val, R_val = make_valid(X, Y, R)\n        candidates.append((np.sum(R_val), X_val, Y_val, R_val))\n\n    # Evaluate sequentially expanding potential utilizing the absolute mathematically precise bound solver directly.\n    candidates.sort(key=lambda x: x[0], reverse=True)\n    \n    # Process only the highest-potential configuration distributions saving overhead structurally ensuring high maximization.\n    for score, X_val, Y_val, R_val in candidates[:10]:\n        X_p, Y_p, R_p = polish_slsqp(X_val, Y_val, R_val)\n        \n        # Reinforcing strict constraints to prevent trace float inaccuracies in automated boundary validations inherently guaranteeing robustness perfectly.\n        X_fin, Y_fin, R_fin = make_valid(X_p, Y_p, R_p)\n        fin_sum = np.sum(R_fin)\n        \n        if fin_sum > best_sum:\n            best_sum = fin_sum\n            best_X = X_fin\n            best_Y = Y_fin\n            best_R = R_fin\n\n    return np.column_stack((best_X, best_Y)), best_R, best_sum\n# EVOLVE-BLOCK-END\n\n# This part remains fixed (not evolved)\ndef run_packing():\n    \"\"\"Run the circle packing constructor for n=26\"\"\"\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii\n\ndef visualize(centers, radii):\n    \"\"\"\n    Visualize the circle packing\n\n    Args:\n        centers: np.array of shape (n, 2) with (x, y) coordinates\n        radii: np.array of shape (n) with radius of each circle\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.patches import Circle\n\n    fig, ax = plt.subplots(figsize=(8, 8))\n\n    # Draw unit square\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_aspect(\"equal\")\n    ax.grid(True)\n\n    # Draw circles\n    for i, (center, radius) in enumerate(zip(centers, radii)):\n        circle = Circle(center, radius, alpha=0.5)\n        ax.add_patch(circle)\n        ax.text(center[0], center[1], str(i), ha=\"center\", va=\"center\")\n\n    plt.title(f\"Circle Packing (n={len(centers)}, sum={sum(radii):.6f})\")\n    plt.show()\n\nif __name__ == \"__main__\":\n    centers, radii, sum_radii = run_packing()\n    print(f\"Sum of radii: {sum_radii}\")\n    # AlphaEvolve improved this to 2.635\n\n    # Uncomment to visualize:\n    # visualize(centers, radii)", "changes_description": null, "language": "python", "parent_id": "2105e72b-3e7a-42bd-a3f5-afd6bec78503", "generation": 2, "timestamp": 1771993091.4808342, "iteration_found": 15, "metrics": {"validity": 1.0, "sum_radii": 2.611625940928049, "target_ratio": 0.9911293893465082, "combined_score": 0.9911293893465082, "radius_variance": 0.006079981284075716, "spatial_spread": 0.18691586184555412, "eval_time": 5.150765895843506}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"validity": 1.0, "sum_radii": 2.559236273881962, "target_ratio": 0.9712471627635529, "combined_score": 0.9712471627635529, "radius_variance": 0.005812544300272144, "spatial_spread": 0.20211324875789904, "eval_time": 6.408116817474365}, "island": 3}, "prompts": {"full_rewrite_user": {"system": "You are an expert mathematician and Python developer specializing in computational geometry and circle packing optimization.\nYour task is to improve the given Python algorithm for finding a highly optimized arrangement of 26 circles within a 1x1 unit square. The objective is to maximize the sum of their radii without any of the circles overlapping or extending outside the boundaries of the square.\nInstead of hardcoding a direct arrangement, implement an iterative optimization algorithm to find the solution.\nWhen designing the optimization routine, incorporate the following geometric heuristics:\n* Seed initial positions strategically: Bias initial placements toward corners and edges to maximize space utilization.\n* Break perfect symmetry: Introduce slight random perturbations during the optimization to escape local maxima caused by edge constraints.\n* Tune optimization parameters: Ensure your physics model uses a decaying learning rate or simulated annealing approach to smoothly settle into the tightest possible packing.\n* Placement by size: consider placing larger circles toward the center and smaller circles into the corners and interstitial gaps.\n\nCode Requirements:\n* Completeness: You MUST provide a fully runnable script. Do not use ellipses (`...`), comments-as-placeholders, or unclosed structures.\n* Style: Ensure the code is syntactically perfect and strictly adheres to PEP 8 formatting.\n* Code review: doublecheck your code to make sure it will not bump into any runtime exceptions.\n* Response Format: response in plain text, without wrapping the code into a Markdown codeblock.\n", "user": "# Current Program Information\n- Fitness: 0.9712\n- Feature coordinates: \n- Focus areas: - Fitness unchanged at 0.9712\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### execution_time\n```\n6.41s\n```\n\n### stage\n```\nquick_validation\n```\n\n### packing_summary\n```\nSum of radii: 2.559236/2.635 = 0.9712\n```\n\n### validation_report\n```\nValid: True, Violations: 0 boundary, 0 overlaps\n```\n\n### stdout\n```\nExcellent packing! Achieved 97.1% of target value\n```\n\n### radius_stats\n```\nMin: 0.060295, Max: 0.133390, Avg: 0.098432\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Full rewrite\n- Metrics: validity: 1.0000, sum_radii: 2.5592, target_ratio: 0.9712, combined_score: 0.9712, radius_variance: 0.0058, spatial_spread: 0.2021, eval_time: 6.4081\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9712)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Constructor-based iterative circle packing optimization for n=26 circles\"\"\"\nimport numpy as np\n\n\ndef get_loss_and_grads(X, Y, R, c_t):\n    \"\"\"\n    Computes penalty-based loss and vectorized gradients to evaluate boundaries and overlaps.\n    Penalty weight `c_t` strictly forces constraints at later stages.\n    \"\"\"\n    grad_X = np.zeros_like(X)\n    grad_Y = np.zeros_like(Y)\n    \n    # We want to maximize sum(R), equivalent to minimizing -sum(R)\n    grad_R = np.full_like(R, -1.0)\n    loss = -np.sum(R)\n    \n    # Square Boundary Restrictions [0, 1]\n    # Penalty ensures circles are fully inside bounds: x-r > 0 => r-x < 0\n    viol_xmin = R - X\n    mask_xmin = viol_xmin > 0\n    if np.any(mask_xmin):\n        grad_R[mask_xmin] += c_t * 2.0 * viol_xmin[mask_xmin]\n        grad_X[mask_xmin] -= c_t * 2.0 * viol_xmin[mask_xmin]\n        loss += c_t * np.sum(viol_xmin[mask_xmin]**2)\n        \n    viol_xmax = X + R - 1.0\n    mask_xmax = viol_xmax > 0\n    if np.any(mask_xmax):\n        grad_R[mask_xmax] += c_t * 2.0 * viol_xmax[mask_xmax]\n        grad_X[mask_xmax] += c_t * 2.0 * viol_xmax[mask_xmax]\n        loss += c_t * np.sum(viol_xmax[mask_xmax]**2)\n        \n    viol_ymin = R - Y\n    mask_ymin = viol_ymin > 0\n    if np.any(mask_ymin):\n        grad_R[mask_ymin] += c_t * 2.0 * viol_ymin[mask_ymin]\n        grad_Y[mask_ymin] -= c_t * 2.0 * viol_ymin[mask_ymin]\n        loss += c_t * np.sum(viol_ymin[mask_ymin]**2)\n        \n    viol_ymax = Y + R - 1.0\n    mask_ymax = viol_ymax > 0\n    if np.any(mask_ymax):\n        grad_R[mask_ymax] += c_t * 2.0 * viol_ymax[mask_ymax]\n        grad_Y[mask_ymax] += c_t * 2.0 * viol_ymax[mask_ymax]\n        loss += c_t * np.sum(viol_ymax[mask_ymax]**2)\n        \n    # Non-overlapping circles Constraints\n    dx = X[:, None] - X[None, :]\n    dy = Y[:, None] - Y[None, :]\n    dist2 = dx**2 + dy**2\n    np.fill_diagonal(dist2, 1.0)\n    \n    # Float stability safeguards ensuring safety from duplicate division\n    dist = np.maximum(np.sqrt(dist2), 1e-10)\n    \n    sum_R = R[:, None] + R[None, :]\n    viol_overlap = sum_R - dist\n    np.fill_diagonal(viol_overlap, -1.0) # ignore self-overlaps\n    \n    mask_overlap = viol_overlap > 0\n    \n    if np.any(mask_overlap):\n        loss += c_t * np.sum(viol_overlap[mask_overlap]**2) / 2.0\n        \n        # Derivatives for pairs\n        factor = c_t * 2.0 * (viol_overlap * mask_overlap)\n        grad_R += np.sum(factor, axis=1)\n        \n        factor_dist = factor / dist\n        grad_X += np.sum(-factor_dist * dx, axis=1)\n        grad_Y += np.sum(-factor_dist * dy, axis=1)\n        \n    return loss, grad_X, grad_Y, grad_R\n\n\ndef make_valid(X, Y, R):\n    \"\"\"\n    Safely forces parameters down onto an exactly strict feasible regime,\n    maintaining position arrangement whilst shrinking only clashing constraints linearly.\n    \"\"\"\n    N = len(R)\n    # Global explicit clamp bounds:\n    R = np.minimum.reduce([R, X, 1.0 - X, Y, 1.0 - Y])\n    R = np.maximum(R, 1e-6)\n    \n    for _ in range(50):\n        dx = X[:, None] - X[None, :]\n        dy = Y[:, None] - Y[None, :]\n        dist = np.sqrt(dx**2 + dy**2)\n        np.fill_diagonal(dist, 10.0)\n        \n        sum_R = R[:, None] + R[None, :]\n        overlap = sum_R - dist\n        \n        if np.max(overlap) <= 1e-8:\n            break\n            \n        indices = np.argsort(overlap.flatten())[::-1]\n        for idx in indices:\n            i, j = divmod(idx, N)\n            if i >= j:\n                continue\n            \n            curr_overlap = R[i] + R[j] - dist[i, j]\n            if curr_overlap > 1e-9:\n                scale = dist[i, j] / (R[i] + R[j])\n                R[i] *= scale\n                R[j] *= scale\n                \n    # Exhaustive final pair-clashing scan ensuring float-truncation exactness\n    for i in range(N):\n        for j in range(i + 1, N):\n            d = np.sqrt((X[i]-X[j])**2 + (Y[i]-Y[j])**2)\n            if R[i] + R[j] > d:\n                scale = d / (R[i] + R[j])\n                R[i] *= scale\n                R[j] *= scale\n                \n    R = np.minimum.reduce([R, X, 1.0 - X, Y, 1.0 - Y])\n    R = np.maximum(R, 0.0)\n    R *= 0.999999 # Micro padding prevents floating inequalities on the boundary test wrapper\n    \n    return X, Y, R\n\n\ndef construct_packing():\n    \"\"\"\n    Implements simulated annealing combined with an explicit multistart unconstrained optimizer.\n    Generates varied topology arrangements natively. \n    Larger target sizes start centrally to create hierarchal Apollonian filling dynamics.\n    \n    Returns:\n        centers, radii, sum_radii safely inside all non-overlapping domain thresholds.\n    \"\"\"\n    best_sum = -1.0\n    best_X = None\n    best_Y = None\n    best_R = None\n    \n    # Extremely robust multistart optimization setup guaranteeing globally resilient optima \n    num_runs = 15\n    N = 26\n    \n    for run in range(num_runs):\n        np.random.seed(42 + run)\n        \n        # Bias placements spatially \n        X_init, Y_init = [], []\n        # Assign seed layout for space efficiency handling edges natively\n        X_init.extend([0.05, 0.95, 0.05, 0.95])\n        Y_init.extend([0.05, 0.05, 0.95, 0.95])\n        \n        # Sequentially scatter randomized fillers\n        for _ in range(N - 4):\n            X_init.append(np.random.uniform(0.1, 0.9))\n            Y_init.append(np.random.uniform(0.1, 0.9))\n            \n        X = np.array(X_init)\n        Y = np.array(Y_init)\n        \n        # Target assignment explicitly favoring centered bodies to push corner circles into tight spaces\n        dist_c = np.sqrt((X - 0.5)**2 + (Y - 0.5)**2)\n        R = 0.06 - 0.04 * dist_c\n        \n        # Smooth simulated annealing learning constraints configuration\n        num_steps = 3000\n        m_X, v_X = np.zeros_like(X), np.zeros_like(X)\n        m_Y, v_Y = np.zeros_like(Y), np.zeros_like(Y)\n        m_R, v_R = np.zeros_like(R), np.zeros_like(R)\n        \n        lr_init = 0.015\n        beta1, beta2 = 0.9, 0.999\n        epsilon = 1e-8\n        \n        # Exponent continuation approach forcing parameters tightly on penalty limits towards end\n        c_t = 10.0\n        c_max = 2000000.0\n        c_mult = (c_max / c_t) ** (1.0 / num_steps)\n        \n        for t in range(1, num_steps + 1):\n            # Adam descent exponentially decreasing standard learning threshold smoothly locking configurations\n            curr_lr = lr_init * (0.01 ** (t / num_steps))\n            \n            _, g_X, g_Y, g_R = get_loss_and_grads(X, Y, R, c_t)\n            \n            # Step updates safely parameterized\n            m_X = beta1 * m_X + (1 - beta1) * g_X\n            v_X = beta2 * v_X + (1 - beta2) * (g_X ** 2)\n            m_X_h = m_X / (1 - beta1 ** t)\n            v_X_h = v_X / (1 - beta2 ** t)\n            X -= curr_lr * m_X_h / (np.sqrt(v_X_h) + epsilon)\n            \n            m_Y = beta1 * m_Y + (1 - beta1) * g_Y\n            v_Y = beta2 * v_Y + (1 - beta2) * (g_Y ** 2)\n            m_Y_h = m_Y / (1 - beta1 ** t)\n            v_Y_h = v_Y / (1 - beta2 ** t)\n            Y -= curr_lr * m_Y_h / (np.sqrt(v_Y_h) + epsilon)\n            \n            m_R = beta1 * m_R + (1 - beta1) * g_R\n            v_R = beta2 * v_R + (1 - beta2) * (g_R ** 2)\n            m_R_h = m_R / (1 - beta1 ** t)\n            v_R_h = v_R / (1 - beta2 ** t)\n            R -= curr_lr * m_R_h / (np.sqrt(v_R_h) + epsilon)\n            \n            X = np.clip(X, 0.0, 1.0)\n            Y = np.clip(Y, 0.0, 1.0)\n            R = np.maximum(R, 0.001)\n            \n            c_t *= c_mult\n            \n            # Sub-stage random topological perturbation jitter overcoming dense local arrangement lockdowns\n            if t % 300 == 0 and t < 1500:\n                X += np.random.normal(0, 0.003, N)\n                Y += np.random.normal(0, 0.003, N)\n                X = np.clip(X, 0.0, 1.0)\n                Y = np.clip(Y, 0.0, 1.0)\n                \n        # Rigid bounds snap strictly to true mathematical feasible regimes unconditionally\n        X_val, Y_val, R_val = make_valid(X, Y, R)\n        curr_sum = np.sum(R_val)\n        \n        if curr_sum > best_sum:\n            best_sum = curr_sum\n            best_X = X_val\n            best_Y = Y_val\n            best_R = R_val\n\n    return np.column_stack((best_X, best_Y)), best_R, best_sum\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef run_packing():\n    \"\"\"Run the circle packing constructor for n=26\"\"\"\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii\n\n\ndef visualize(centers, radii):\n    \"\"\"\n    Visualize the circle packing\n\n    Args:\n        centers: np.array of shape (n, 2) with (x, y) coordinates\n        radii: np.array of shape (n) with radius of each circle\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.patches import Circle\n\n    fig, ax = plt.subplots(figsize=(8, 8))\n\n    # Draw unit square\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_aspect(\"equal\")\n    ax.grid(True)\n\n    # Draw circles\n    for i, (center, radius) in enumerate(zip(centers, radii)):\n        circle = Circle(center, radius, alpha=0.5)\n        ax.add_patch(circle)\n        ax.text(center[0], center[1], str(i), ha=\"center\", va=\"center\")\n\n    plt.title(f\"Circle Packing (n={len(centers)}, sum={sum(radii):.6f})\")\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    centers, radii, sum_radii = run_packing()\n    print(f\"Sum of radii: {sum_radii}\")\n    # AlphaEvolve improved this to 2.635\n\n    # Uncomment to visualize:\n    # visualize(centers, radii)\n```\nKey features: Performs well on validity (1.0000), Performs well on sum_radii (2.5592), Performs well on target_ratio (0.9712), Performs well on combined_score (0.9712), Performs well on radius_variance (0.0058), Performs well on spatial_spread (0.2021), Performs well on eval_time (6.4081)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Constructor-based iterative circle packing optimization for n=26 circles\"\"\"\nimport numpy as np\n\n\ndef get_loss_and_grads(X, Y, R, c_t):\n    \"\"\"\n    Computes penalty-based loss and vectorized gradients to evaluate boundaries and overlaps.\n    Penalty weight `c_t` strictly forces constraints at later stages.\n    \"\"\"\n    grad_X = np.zeros_like(X)\n    grad_Y = np.zeros_like(Y)\n    \n    # We want to maximize sum(R), equivalent to minimizing -sum(R)\n    grad_R = np.full_like(R, -1.0)\n    loss = -np.sum(R)\n    \n    # Square Boundary Restrictions [0, 1]\n    # Penalty ensures circles are fully inside bounds: x-r > 0 => r-x < 0\n    viol_xmin = R - X\n    mask_xmin = viol_xmin > 0\n    if np.any(mask_xmin):\n        grad_R[mask_xmin] += c_t * 2.0 * viol_xmin[mask_xmin]\n        grad_X[mask_xmin] -= c_t * 2.0 * viol_xmin[mask_xmin]\n        loss += c_t * np.sum(viol_xmin[mask_xmin]**2)\n        \n    viol_xmax = X + R - 1.0\n    mask_xmax = viol_xmax > 0\n    if np.any(mask_xmax):\n        grad_R[mask_xmax] += c_t * 2.0 * viol_xmax[mask_xmax]\n        grad_X[mask_xmax] += c_t * 2.0 * viol_xmax[mask_xmax]\n        loss += c_t * np.sum(viol_xmax[mask_xmax]**2)\n        \n    viol_ymin = R - Y\n    mask_ymin = viol_ymin > 0\n    if np.any(mask_ymin):\n        grad_R[mask_ymin] += c_t * 2.0 * viol_ymin[mask_ymin]\n        grad_Y[mask_ymin] -= c_t * 2.0 * viol_ymin[mask_ymin]\n        loss += c_t * np.sum(viol_ymin[mask_ymin]**2)\n        \n    viol_ymax = Y + R - 1.0\n    mask_ymax = viol_ymax > 0\n    if np.any(mask_ymax):\n        grad_R[mask_ymax] += c_t * 2.0 * viol_ymax[mask_ymax]\n        grad_Y[mask_ymax] += c_t * 2.0 * viol_ymax[mask_ymax]\n        loss += c_t * np.sum(viol_ymax[mask_ymax]**2)\n        \n    # Non-overlapping circles Constraints\n    dx = X[:, None] - X[None, :]\n    dy = Y[:, None] - Y[None, :]\n    dist2 = dx**2 + dy**2\n    np.fill_diagonal(dist2, 1.0)\n    \n    # Float stability safeguards ensuring safety from duplicate division\n    dist = np.maximum(np.sqrt(dist2), 1e-10)\n    \n    sum_R = R[:, None] + R[None, :]\n    viol_overlap = sum_R - dist\n    np.fill_diagonal(viol_overlap, -1.0) # ignore self-overlaps\n    \n    mask_overlap = viol_overlap > 0\n    \n    if np.any(mask_overlap):\n        loss += c_t * np.sum(viol_overlap[mask_overlap]**2) / 2.0\n        \n        # Derivatives for pairs\n        factor = c_t * 2.0 * (viol_overlap * mask_overlap)\n        grad_R += np.sum(factor, axis=1)\n        \n        factor_dist = factor / dist\n        grad_X += np.sum(-factor_dist * dx, axis=1)\n        grad_Y += np.sum(-factor_dist * dy, axis=1)\n        \n    return loss, grad_X, grad_Y, grad_R\n\n\ndef make_valid(X, Y, R):\n    \"\"\"\n    Safely forces parameters down onto an exactly strict feasible regime,\n    maintaining position arrangement whilst shrinking only clashing constraints linearly.\n    \"\"\"\n    N = len(R)\n    # Global explicit clamp bounds:\n    R = np.minimum.reduce([R, X, 1.0 - X, Y, 1.0 - Y])\n    R = np.maximum(R, 1e-6)\n    \n    for _ in range(50):\n        dx = X[:, None] - X[None, :]\n        dy = Y[:, None] - Y[None, :]\n        dist = np.sqrt(dx**2 + dy**2)\n        np.fill_diagonal(dist, 10.0)\n        \n        sum_R = R[:, None] + R[None, :]\n        overlap = sum_R - dist\n        \n        if np.max(overlap) <= 1e-8:\n            break\n            \n        indices = np.argsort(overlap.flatten())[::-1]\n        for idx in indices:\n            i, j = divmod(idx, N)\n            if i >= j:\n                continue\n            \n            curr_overlap = R[i] + R[j] - dist[i, j]\n            if curr_overlap > 1e-9:\n                scale = dist[i, j] / (R[i] + R[j])\n                R[i] *= scale\n                R[j] *= scale\n                \n    # Exhaustive final pair-clashing scan ensuring float-truncation exactness\n    for i in range(N):\n        for j in range(i + 1, N):\n            d = np.sqrt((X[i]-X[j])**2 + (Y[i]-Y[j])**2)\n            if R[i] + R[j] > d:\n                scale = d / (R[i] + R[j])\n                R[i] *= scale\n                R[j] *= scale\n                \n    R = np.minimum.reduce([R, X, 1.0 - X, Y, 1.0 - Y])\n    R = np.maximum(R, 0.0)\n    R *= 0.999999 # Micro padding prevents floating inequalities on the boundary test wrapper\n    \n    return X, Y, R\n\n\ndef construct_packing():\n    \"\"\"\n    Implements simulated annealing combined with an explicit multistart unconstrained optimizer.\n    Generates varied topology arrangements natively. \n    Larger target sizes start centrally to create hierarchal Apollonian filling dynamics.\n    \n    Returns:\n        centers, radii, sum_radii safely inside all non-overlapping domain thresholds.\n    \"\"\"\n    best_sum = -1.0\n    best_X = None\n    best_Y = None\n    best_R = None\n    \n    # Extremely robust multistart optimization setup guaranteeing globally resilient optima \n    num_runs = 15\n    N = 26\n    \n    for run in range(num_runs):\n        np.random.seed(42 + run)\n        \n        # Bias placements spatially \n        X_init, Y_init = [], []\n        # Assign seed layout for space efficiency handling edges natively\n        X_init.extend([0.05, 0.95, 0.05, 0.95])\n        Y_init.extend([0.05, 0.05, 0.95, 0.95])\n        \n        # Sequentially scatter randomized fillers\n        for _ in range(N - 4):\n            X_init.append(np.random.uniform(0.1, 0.9))\n            Y_init.append(np.random.uniform(0.1, 0.9))\n            \n        X = np.array(X_init)\n        Y = np.array(Y_init)\n        \n        # Target assignment explicitly favoring centered bodies to push corner circles into tight spaces\n        dist_c = np.sqrt((X - 0.5)**2 + (Y - 0.5)**2)\n        R = 0.06 - 0.04 * dist_c\n        \n        # Smooth simulated annealing learning constraints configuration\n        num_steps = 3000\n        m_X, v_X = np.zeros_like(X), np.zeros_like(X)\n        m_Y, v_Y = np.zeros_like(Y), np.zeros_like(Y)\n        m_R, v_R = np.zeros_like(R), np.zeros_like(R)\n        \n        lr_init = 0.015\n        beta1, beta2 = 0.9, 0.999\n        epsilon = 1e-8\n        \n        # Exponent continuation approach forcing parameters tightly on penalty limits towards end\n        c_t = 10.0\n        c_max = 2000000.0\n        c_mult = (c_max / c_t) ** (1.0 / num_steps)\n        \n        for t in range(1, num_steps + 1):\n            # Adam descent exponentially decreasing standard learning threshold smoothly locking configurations\n            curr_lr = lr_init * (0.01 ** (t / num_steps))\n            \n            _, g_X, g_Y, g_R = get_loss_and_grads(X, Y, R, c_t)\n            \n            # Step updates safely parameterized\n            m_X = beta1 * m_X + (1 - beta1) * g_X\n            v_X = beta2 * v_X + (1 - beta2) * (g_X ** 2)\n            m_X_h = m_X / (1 - beta1 ** t)\n            v_X_h = v_X / (1 - beta2 ** t)\n            X -= curr_lr * m_X_h / (np.sqrt(v_X_h) + epsilon)\n            \n            m_Y = beta1 * m_Y + (1 - beta1) * g_Y\n            v_Y = beta2 * v_Y + (1 - beta2) * (g_Y ** 2)\n            m_Y_h = m_Y / (1 - beta1 ** t)\n            v_Y_h = v_Y / (1 - beta2 ** t)\n            Y -= curr_lr * m_Y_h / (np.sqrt(v_Y_h) + epsilon)\n            \n            m_R = beta1 * m_R + (1 - beta1) * g_R\n            v_R = beta2 * v_R + (1 - beta2) * (g_R ** 2)\n            m_R_h = m_R / (1 - beta1 ** t)\n            v_R_h = v_R / (1 - beta2 ** t)\n            R -= curr_lr * m_R_h / (np.sqrt(v_R_h) + epsilon)\n            \n            X = np.clip(X, 0.0, 1.0)\n            Y = np.clip(Y, 0.0, 1.0)\n            R = np.maximum(R, 0.001)\n            \n            c_t *= c_mult\n            \n            # Sub-stage random topological perturbation jitter overcoming dense local arrangement lockdowns\n            if t % 300 == 0 and t < 1500:\n                X += np.random.normal(0, 0.003, N)\n                Y += np.random.normal(0, 0.003, N)\n                X = np.clip(X, 0.0, 1.0)\n                Y = np.clip(Y, 0.0, 1.0)\n                \n        # Rigid bounds snap strictly to true mathematical feasible regimes unconditionally\n        X_val, Y_val, R_val = make_valid(X, Y, R)\n        curr_sum = np.sum(R_val)\n        \n        if curr_sum > best_sum:\n            best_sum = curr_sum\n            best_X = X_val\n            best_Y = Y_val\n            best_R = R_val\n\n    return np.column_stack((best_X, best_Y)), best_R, best_sum\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef run_packing():\n    \"\"\"Run the circle packing constructor for n=26\"\"\"\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii\n\n\ndef visualize(centers, radii):\n    \"\"\"\n    Visualize the circle packing\n\n    Args:\n        centers: np.array of shape (n, 2) with (x, y) coordinates\n        radii: np.array of shape (n) with radius of each circle\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.patches import Circle\n\n    fig, ax = plt.subplots(figsize=(8, 8))\n\n    # Draw unit square\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_aspect(\"equal\")\n    ax.grid(True)\n\n    # Draw circles\n    for i, (center, radius) in enumerate(zip(centers, radii)):\n        circle = Circle(center, radius, alpha=0.5)\n        ax.add_patch(circle)\n        ax.text(center[0], center[1], str(i), ha=\"center\", va=\"center\")\n\n    plt.title(f\"Circle Packing (n={len(centers)}, sum={sum(radii):.6f})\")\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    centers, radii, sum_radii = run_packing()\n    print(f\"Sum of radii: {sum_radii}\")\n    # AlphaEvolve improved this to 2.635\n\n    # Uncomment to visualize:\n    # visualize(centers, radii)\n```\n\n# Task\nRewrite the program to improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```", "responses": ["```python\n# EVOLVE-BLOCK-START\n\"\"\"Constructor-based iterative circle packing optimization for n=26 circles\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\n\ndef get_loss_and_grads(X, Y, R, c_t):\n    \"\"\"\n    Computes penalty-based loss and vectorized gradients to evaluate boundaries and overlaps.\n    Penalty weight `c_t` strictly forces constraints at later stages to create proper layouts.\n    \"\"\"\n    grad_X = np.zeros_like(X)\n    grad_Y = np.zeros_like(Y)\n    \n    # Maximize sum(R) -> Equivalent to minimizing -sum(R)\n    grad_R = np.full_like(R, -1.0)\n    loss = -np.sum(R)\n    \n    # Square Boundary Restrictions [0, 1]\n    viol_xmin = R - X\n    mask_xmin = viol_xmin > 0\n    if np.any(mask_xmin):\n        grad_R[mask_xmin] += c_t * 2.0 * viol_xmin[mask_xmin]\n        grad_X[mask_xmin] -= c_t * 2.0 * viol_xmin[mask_xmin]\n        loss += c_t * np.sum(viol_xmin[mask_xmin]**2)\n        \n    viol_xmax = X + R - 1.0\n    mask_xmax = viol_xmax > 0\n    if np.any(mask_xmax):\n        grad_R[mask_xmax] += c_t * 2.0 * viol_xmax[mask_xmax]\n        grad_X[mask_xmax] += c_t * 2.0 * viol_xmax[mask_xmax]\n        loss += c_t * np.sum(viol_xmax[mask_xmax]**2)\n        \n    viol_ymin = R - Y\n    mask_ymin = viol_ymin > 0\n    if np.any(mask_ymin):\n        grad_R[mask_ymin] += c_t * 2.0 * viol_ymin[mask_ymin]\n        grad_Y[mask_ymin] -= c_t * 2.0 * viol_ymin[mask_ymin]\n        loss += c_t * np.sum(viol_ymin[mask_ymin]**2)\n        \n    viol_ymax = Y + R - 1.0\n    mask_ymax = viol_ymax > 0\n    if np.any(mask_ymax):\n        grad_R[mask_ymax] += c_t * 2.0 * viol_ymax[mask_ymax]\n        grad_Y[mask_ymax] += c_t * 2.0 * viol_ymax[mask_ymax]\n        loss += c_t * np.sum(viol_ymax[mask_ymax]**2)\n        \n    # Non-overlapping circles Constraints\n    dx = X[:, None] - X[None, :]\n    dy = Y[:, None] - Y[None, :]\n    dist2 = dx**2 + dy**2\n    np.fill_diagonal(dist2, 1.0)\n    \n    # Float stability safeguards\n    dist = np.maximum(np.sqrt(dist2), 1e-10)\n    \n    sum_R = R[:, None] + R[None, :]\n    viol_overlap = sum_R - dist\n    np.fill_diagonal(viol_overlap, -1.0) # ignore self-overlaps\n    \n    mask_overlap = viol_overlap > 0\n    \n    if np.any(mask_overlap):\n        loss += c_t * np.sum(viol_overlap[mask_overlap]**2) / 2.0\n        \n        # Derivatives for pairs\n        factor = c_t * 2.0 * (viol_overlap * mask_overlap)\n        grad_R += np.sum(factor, axis=1)\n        \n        factor_dist = factor / dist\n        grad_X += np.sum(-factor_dist * dx, axis=1)\n        grad_Y += np.sum(-factor_dist * dy, axis=1)\n        \n    return loss, grad_X, grad_Y, grad_R\n\n\ndef make_valid(X, Y, R):\n    \"\"\"\n    Safely forces parameters down onto an exactly strict feasible regime mathematically,\n    shrinking only physically colliding or bound-infringing objects efficiently.\n    \"\"\"\n    N = len(R)\n    # Global explicit clamp bounds:\n    R = np.minimum.reduce([R, X, 1.0 - X, Y, 1.0 - Y])\n    R = np.maximum(R, 1e-6)\n    \n    for _ in range(50):\n        dx = X[:, None] - X[None, :]\n        dy = Y[:, None] - Y[None, :]\n        dist = np.sqrt(dx**2 + dy**2)\n        np.fill_diagonal(dist, 10.0)\n        \n        sum_R = R[:, None] + R[None, :]\n        overlap = sum_R - dist\n        \n        if np.max(overlap) <= 1e-8:\n            break\n            \n        indices = np.argsort(overlap.flatten())[::-1]\n        for idx in indices:\n            i, j = divmod(int(idx), N)\n            if i >= j:\n                continue\n            \n            curr_overlap = R[i] + R[j] - dist[i, j]\n            if curr_overlap > 1e-9:\n                scale = dist[i, j] / (R[i] + R[j])\n                R[i] *= scale\n                R[j] *= scale\n                \n    # Exhaustive final pair-clashing scan ensuring absolute float-truncation constraint truth\n    for i in range(N):\n        for j in range(i + 1, N):\n            d = np.sqrt((X[i]-X[j])**2 + (Y[i]-Y[j])**2)\n            if R[i] + R[j] > d:\n                scale = d / (R[i] + R[j])\n                R[i] *= scale\n                R[j] *= scale\n                \n    R = np.minimum.reduce([R, X, 1.0 - X, Y, 1.0 - Y])\n    R = np.maximum(R, 0.0)\n    R *= 0.999999 # Micro padding entirely offsets precision drift\n    \n    return X, Y, R\n\n\ndef polish_slsqp(X, Y, R):\n    \"\"\"\n    Fine-tunes mathematically feasible solutions continuously into strict optima.\n    Utilizes localized L-BFGS constrained projections heavily pushing radius magnitudes upwards\n    towards maximum configuration geometries smoothly against identical local configurations.\n    \"\"\"\n    N = len(R)\n    \n    def obj(p):\n        return -np.sum(p[2*N:])\n        \n    def grad_obj(p):\n        g = np.zeros(3*N)\n        g[2*N:] = -1.0\n        return g\n        \n    bounds = [(0.0, 1.0)] * (2*N) + [(0.001, 1.0)] * N\n    \n    def boundary_cons(p):\n        X_p = p[:N]\n        Y_p = p[N:2*N]\n        R_p = p[2*N:]\n        return np.concatenate([\n            X_p - R_p,\n            1.0 - X_p - R_p,\n            Y_p - R_p,\n            1.0 - Y_p - R_p\n        ])\n        \n    def boundary_jac(p):\n        J = np.zeros((4*N, 3*N))\n        idx = np.arange(N)\n        # Vectorized assignment representing explicit border constraint components respectively\n        J[idx, idx] = 1.0\n        J[idx, 2*N + idx] = -1.0\n        J[N + idx, idx] = -1.0\n        J[N + idx, 2*N + idx] = -1.0\n        J[2*N + idx, N + idx] = 1.0\n        J[2*N + idx, 2*N + idx] = -1.0\n        J[3*N + idx, N + idx] = -1.0\n        J[3*N + idx, 2*N + idx] = -1.0\n        return J\n\n    def overlap_cons(p):\n        X_p = p[:N]\n        Y_p = p[N:2*N]\n        R_p = p[2*N:]\n        i_idx, j_idx = np.triu_indices(N, k=1)\n        dist2 = (X_p[i_idx] - X_p[j_idx])**2 + (Y_p[i_idx] - Y_p[j_idx])**2\n        return dist2 - (R_p[i_idx] + R_p[j_idx])**2\n        \n    def overlap_jac(p):\n        X_p = p[:N]\n        Y_p = p[N:2*N]\n        R_p = p[2*N:]\n        i_idx, j_idx = np.triu_indices(N, k=1)\n        num_pairs = len(i_idx)\n        J = np.zeros((num_pairs, 3*N))\n        \n        dx = X_p[i_idx] - X_p[j_idx]\n        dy = Y_p[i_idx] - Y_p[j_idx]\n        dr = R_p[i_idx] + R_p[j_idx]\n        \n        pair_range = np.arange(num_pairs)\n        J[pair_range, i_idx] = 2.0 * dx\n        J[pair_range, j_idx] = -2.0 * dx\n        J[pair_range, N + i_idx] = 2.0 * dy\n        J[pair_range, N + j_idx] = -2.0 * dy\n        J[pair_range, 2*N + i_idx] = -2.0 * dr\n        J[pair_range, 2*N + j_idx] = -2.0 * dr\n        return J\n\n    cons = [\n        {'type': 'ineq', 'fun': boundary_cons, 'jac': boundary_jac},\n        {'type': 'ineq', 'fun': overlap_cons, 'jac': overlap_jac}\n    ]\n    \n    p0 = np.concatenate([X, Y, R])\n    res = minimize(\n        obj, p0, method='SLSQP', jac=grad_obj, bounds=bounds,\n        constraints=cons, options={'maxiter': 400, 'ftol': 1e-6}\n    )\n    \n    p = res.x\n    return p[:N], p[N:2*N], p[2*N:]\n\n\ndef construct_packing():\n    \"\"\"\n    Implements simulated annealing optimization combined with distinct symmetry-breaking seeds.\n    Final solutions iteratively passed through deterministic physical expansion maximizations utilizing SQP bounds mapping natively.\n    \"\"\"\n    best_sum = -1.0\n    best_X = None\n    best_Y = None\n    best_R = None\n    \n    num_runs = 16\n    N = 26\n    candidates = []\n    \n    for run in range(num_runs):\n        np.random.seed(42 + run)\n        \n        # Sequentially map distinctly varied geometric layout arrangements forcing extensive diverse searches initially\n        X_init, Y_init = [], []\n        if run % 4 == 0:\n            X_init.extend([0.05, 0.95, 0.05, 0.95])\n            Y_init.extend([0.05, 0.05, 0.95, 0.95])\n            for _ in range(N - 4):\n                X_init.append(np.random.uniform(0.1, 0.9))\n                Y_init.append(np.random.uniform(0.1, 0.9))\n        elif run % 4 == 1:\n            for _ in range(N):\n                X_init.append(np.random.uniform(0.05, 0.95))\n                Y_init.append(np.random.uniform(0.05, 0.95))\n        elif run % 4 == 2:\n            n_outer = 12\n            for i in range(n_outer):\n                th = 2 * np.pi * i / n_outer\n                X_init.append(0.5 + 0.38 * np.cos(th))\n                Y_init.append(0.5 + 0.38 * np.sin(th))\n            for _ in range(N - n_outer):\n                X_init.append(np.random.uniform(0.25, 0.75))\n                Y_init.append(np.random.uniform(0.25, 0.75))\n        else:\n            grid_dim = int(np.ceil(np.sqrt(N)))\n            for i in range(N):\n                r_pos = i // grid_dim\n                c_pos = i % grid_dim\n                X_init.append(0.1 + 0.8 * r_pos / max(1, grid_dim - 1))\n                Y_init.append(0.1 + 0.8 * c_pos / max(1, grid_dim - 1))\n            \n        X = np.array(X_init[:N])\n        Y = np.array(Y_init[:N])\n        \n        if run % 2 == 0:\n            dist_c = np.sqrt((X - 0.5)**2 + (Y - 0.5)**2)\n            R = 0.06 - 0.04 * dist_c\n        else:\n            R = np.random.uniform(0.02, 0.06, N)\n        \n        # Adaptive gradient descent hyperparameters structurally locking positions down gracefully.\n        num_steps = 2000\n        m_X, v_X = np.zeros_like(X), np.zeros_like(X)\n        m_Y, v_Y = np.zeros_like(Y), np.zeros_like(Y)\n        m_R, v_R = np.zeros_like(R), np.zeros_like(R)\n        \n        lr_init = 0.015\n        beta1, beta2 = 0.9, 0.999\n        epsilon = 1e-8\n        \n        c_t = 10.0\n        c_max = 500000.0 \n        c_mult = (c_max / c_t) ** (1.0 / num_steps)\n        \n        for t in range(1, num_steps + 1):\n            curr_lr = lr_init * (0.01 ** (t / num_steps))\n            \n            _, g_X, g_Y, g_R = get_loss_and_grads(X, Y, R, c_t)\n            \n            m_X = beta1 * m_X + (1 - beta1) * g_X\n            v_X = beta2 * v_X + (1 - beta2) * (g_X ** 2)\n            m_X_h = m_X / (1 - beta1 ** t)\n            v_X_h = v_X / (1 - beta2 ** t)\n            X -= curr_lr * m_X_h / (np.sqrt(v_X_h) + epsilon)\n            \n            m_Y = beta1 * m_Y + (1 - beta1) * g_Y\n            v_Y = beta2 * v_Y + (1 - beta2) * (g_Y ** 2)\n            m_Y_h = m_Y / (1 - beta1 ** t)\n            v_Y_h = v_Y / (1 - beta2 ** t)\n            Y -= curr_lr * m_Y_h / (np.sqrt(v_Y_h) + epsilon)\n            \n            m_R = beta1 * m_R + (1 - beta1) * g_R\n            v_R = beta2 * v_R + (1 - beta2) * (g_R ** 2)\n            m_R_h = m_R / (1 - beta1 ** t)\n            v_R_h = v_R / (1 - beta2 ** t)\n            R -= curr_lr * m_R_h / (np.sqrt(v_R_h) + epsilon)\n            \n            X = np.clip(X, 0.0, 1.0)\n            Y = np.clip(Y, 0.0, 1.0)\n            R = np.maximum(R, 0.001)\n            \n            c_t *= c_mult\n            \n            # Sub-stage random topological perturbation jitter dynamically preventing edge-constraint trapping effectively natively\n            if t % 300 == 0 and t < 1200:\n                X += np.random.normal(0, 0.002, N)\n                Y += np.random.normal(0, 0.002, N)\n                X = np.clip(X, 0.0, 1.0)\n                Y = np.clip(Y, 0.0, 1.0)\n                \n        # Rigid bounds cleanly formatting true feasibility states conditionally ready to deploy strictly via refinement\n        X_val, Y_val, R_val = make_valid(X, Y, R)\n        candidates.append((np.sum(R_val), X_val, Y_val, R_val))\n\n    # Evaluate sequentially expanding potential utilizing the absolute mathematically precise bound solver directly.\n    candidates.sort(key=lambda x: x[0], reverse=True)\n    \n    # Process only the highest-potential configuration distributions saving overhead structurally ensuring high maximization.\n    for score, X_val, Y_val, R_val in candidates[:10]:\n        X_p, Y_p, R_p = polish_slsqp(X_val, Y_val, R_val)\n        \n        # Reinforcing strict constraints to prevent trace float inaccuracies in automated boundary validations inherently guaranteeing robustness perfectly.\n        X_fin, Y_fin, R_fin = make_valid(X_p, Y_p, R_p)\n        fin_sum = np.sum(R_fin)\n        \n        if fin_sum > best_sum:\n            best_sum = fin_sum\n            best_X = X_fin\n            best_Y = Y_fin\n            best_R = R_fin\n\n    return np.column_stack((best_X, best_Y)), best_R, best_sum\n# EVOLVE-BLOCK-END\n\n# This part remains fixed (not evolved)\ndef run_packing():\n    \"\"\"Run the circle packing constructor for n=26\"\"\"\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii\n\ndef visualize(centers, radii):\n    \"\"\"\n    Visualize the circle packing\n\n    Args:\n        centers: np.array of shape (n, 2) with (x, y) coordinates\n        radii: np.array of shape (n) with radius of each circle\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.patches import Circle\n\n    fig, ax = plt.subplots(figsize=(8, 8))\n\n    # Draw unit square\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_aspect(\"equal\")\n    ax.grid(True)\n\n    # Draw circles\n    for i, (center, radius) in enumerate(zip(centers, radii)):\n        circle = Circle(center, radius, alpha=0.5)\n        ax.add_patch(circle)\n        ax.text(center[0], center[1], str(i), ha=\"center\", va=\"center\")\n\n    plt.title(f\"Circle Packing (n={len(centers)}, sum={sum(radii):.6f})\")\n    plt.show()\n\nif __name__ == \"__main__\":\n    centers, radii, sum_radii = run_packing()\n    print(f\"Sum of radii: {sum_radii}\")\n    # AlphaEvolve improved this to 2.635\n\n    # Uncomment to visualize:\n    # visualize(centers, radii)\n```"]}}, "artifacts_json": "{\"execution_time\": \"5.15s\", \"stage\": \"quick_validation\", \"packing_summary\": \"Sum of radii: 2.611626/2.635 = 0.9911\", \"validation_report\": \"Valid: True, Violations: 0 boundary, 0 overlaps\", \"stdout\": \"Excellent packing! Achieved 99.1% of target value\", \"radius_stats\": \"Min: 0.061411, Max: 0.135971, Avg: 0.100447\"}", "artifact_dir": null, "embedding": null}